{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8900860d-c565-4843-9848-9149eaa7f8e4",
   "metadata": {},
   "source": [
    "# Dataset ingestion\n",
    "\n",
    "This jupyter noteebook ingests the [Geocoded National Address File][gnaf] ([GNAF][gnaf]) from [data.gov.au](data.gov.au). It also downloads the [land values for NSW][nswlv], and ABS shapefiles \n",
    "\n",
    "It loads it all this data into a PostgreSQL database in a docker container, treating it like a disposable sqlite data store. It also downloads the ABS shape files as well as the \n",
    "\n",
    "Here we are going to ingest all the data necessary in order to assess land by land values, and filter them by address information. \n",
    "\n",
    "### The Steps\n",
    "\n",
    "1. Download static assets and datasets\n",
    "2. Setup a docker container with postgresql with GIS capabilities.\n",
    "3. Ingest the [ABS shape files][abssf]\n",
    "4. Ingest the latest [NSW valuer general land values][nswlv].\n",
    "5. Ingest the [Geocoded National Address File][gnaf] ([GNAF][gnaf]) dataset\n",
    "6. Link NSW Valuer General data with GNAF dataset\n",
    "\n",
    "[gnaf]: https://data.gov.au/data/dataset/geocoded-national-address-file-g-naf\n",
    "[nswlv]: https://www.valuergeneral.nsw.gov.au/land_value_summaries/lv.php\n",
    "[abssf]: https://www.abs.gov.au/statistics/standards/australian-statistical-geography-standard-asgs-edition-3/jul2021-jun2026/access-and-downloads/digital-boundary-files\n",
    "\n",
    "### Note\n",
    "\n",
    "- Make sure docker is running first.\n",
    "\n",
    "### Warning\n",
    "\n",
    "Do not connect this to another database unless you've taken the time to update this, as it'll drop the existing database. I suggest instead take what you need from this script and disregard the rest. DO NOT USE DATABASE CREDENTIALS HERE FOR ANY OTHER STORE (especailly anything with drop permissions).\n",
    "\n",
    "It also executes sql from a zip file downloaded from an external source.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70d4c59-4377-41e7-8143-6d1f46817480",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "These are some fields to configure if you wish to configure how the data is injected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f5c21a9-9440-40ac-a2c7-b20f31444ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import notebook_constants as nc\n",
    "\n",
    "# If you mark this as true, the table `nsw_valuer_general.raw_entries`\n",
    "# will be dropped. If you have space limitations and no desire to debug\n",
    "# the data than dropping this makes sense. If you wish to debug some values\n",
    "# then keeping this around may make some sense.\n",
    "GLOBAL_FLAGS = {\n",
    "    'drop_raw_nsw_valuer_general_entries': True,\n",
    "}\n",
    "\n",
    "db_conf = nc.gnaf_dbconf_2\n",
    "db_name = nc.gnaf_dbname_2\n",
    "\n",
    "docker_container_name = 'gnaf_db_test'\n",
    "docker_image_tag = \"20240908_19_53\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e529c6-4590-4070-88f8-83693e1ef682",
   "metadata": {},
   "source": [
    "## Download Static Files\n",
    "\n",
    "Here we are downloading static files, as well as fetching the most recently published land values from the valuer generals website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "320cd7fe-f03d-4230-9e0f-a2ef302a91b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gnaf-2020.zip\n",
      "Checking non_abs_shape.zip\n",
      "Checking cities.zip\n",
      "Checking g-naf_aug24_allstates_gda2020_psv_1016.zip\n",
      "Checking nswvg_lv_01_Sep_2024.zip\n",
      "Checking nswvg_wps_01_Jan_2024.zip\n",
      "Checking nswvg_wps_08_Jan_2024.zip\n",
      "Checking nswvg_wps_15_Jan_2024.zip\n",
      "Checking nswvg_wps_22_Jan_2024.zip\n",
      "Checking nswvg_wps_29_Jan_2024.zip\n",
      "Checking nswvg_wps_05_Feb_2024.zip\n",
      "Checking nswvg_wps_12_Feb_2024.zip\n",
      "Checking nswvg_wps_19_Feb_2024.zip\n",
      "Checking nswvg_wps_26_Feb_2024.zip\n",
      "Checking nswvg_wps_04_Mar_2024.zip\n",
      "Checking nswvg_wps_11_Mar_2024.zip\n",
      "Checking nswvg_wps_18_Mar_2024.zip\n",
      "Checking nswvg_wps_25_Mar_2024.zip\n",
      "Checking nswvg_wps_01_Apr_2024.zip\n",
      "Checking nswvg_wps_08_Apr_2024.zip\n",
      "Checking nswvg_wps_15_Apr_2024.zip\n",
      "Checking nswvg_wps_22_Apr_2024.zip\n",
      "Checking nswvg_wps_29_Apr_2024.zip\n",
      "Checking nswvg_wps_06_May_2024.zip\n",
      "Checking nswvg_wps_13_May_2024.zip\n",
      "Checking nswvg_wps_20_May_2024.zip\n",
      "Checking nswvg_wps_27_May_2024.zip\n",
      "Checking nswvg_wps_03_Jun_2024.zip\n",
      "Checking nswvg_wps_10_Jun_2024.zip\n",
      "Checking nswvg_wps_17_Jun_2024.zip\n",
      "Checking nswvg_wps_24_Jun_2024.zip\n",
      "Checking nswvg_wps_01_Jul_2024.zip\n",
      "Checking nswvg_wps_08_Jul_2024.zip\n",
      "Checking nswvg_wps_15_Jul_2024.zip\n",
      "Checking nswvg_wps_22_Jul_2024.zip\n",
      "Checking nswvg_wps_29_Jul_2024.zip\n",
      "Checking nswvg_wps_05_Aug_2024.zip\n",
      "Checking nswvg_wps_12_Aug_2024.zip\n",
      "Checking nswvg_wps_19_Aug_2024.zip\n",
      "Checking nswvg_wps_26_Aug_2024.zip\n",
      "Checking nswvg_wps_02_Sep_2024.zip\n",
      "Checking nswvg_aps_1990.zip\n",
      "Checking nswvg_aps_1991.zip\n",
      "Checking nswvg_aps_1992.zip\n",
      "Checking nswvg_aps_1993.zip\n",
      "Checking nswvg_aps_1994.zip\n",
      "Checking nswvg_aps_1995.zip\n",
      "Checking nswvg_aps_1996.zip\n",
      "Checking nswvg_aps_1997.zip\n",
      "Checking nswvg_aps_1998.zip\n",
      "Checking nswvg_aps_1999.zip\n",
      "Checking nswvg_aps_2000.zip\n",
      "Checking nswvg_aps_2001.zip\n",
      "Checking nswvg_aps_2002.zip\n",
      "Checking nswvg_aps_2003.zip\n",
      "Checking nswvg_aps_2004.zip\n",
      "Checking nswvg_aps_2005.zip\n",
      "Checking nswvg_aps_2006.zip\n",
      "Checking nswvg_aps_2007.zip\n",
      "Checking nswvg_aps_2008.zip\n",
      "Checking nswvg_aps_2009.zip\n",
      "Checking nswvg_aps_2010.zip\n",
      "Checking nswvg_aps_2011.zip\n",
      "Checking nswvg_aps_2012.zip\n",
      "Checking nswvg_aps_2013.zip\n",
      "Checking nswvg_aps_2014.zip\n",
      "Checking nswvg_aps_2015.zip\n",
      "Checking nswvg_aps_2016.zip\n",
      "Checking nswvg_aps_2017.zip\n",
      "Checking nswvg_aps_2018.zip\n",
      "Checking nswvg_aps_2019.zip\n",
      "Checking nswvg_aps_2020.zip\n",
      "Checking nswvg_aps_2021.zip\n",
      "Checking nswvg_aps_2022.zip\n",
      "Checking nswvg_aps_2023.zip\n"
     ]
    }
   ],
   "source": [
    "from lib.gnaf.discovery import GnafPublicationDiscovery\n",
    "from lib.nsw_vg.discovery import WeeklySalePriceDiscovery, AnnualSalePriceDiscovery, LandValueDiscovery\n",
    "from lib.remote_resources import StaticFileInitialiser\n",
    "\n",
    "initialiser = StaticFileInitialiser.create()\n",
    "\n",
    "land_value = LandValueDiscovery()\n",
    "w_sale_price = WeeklySalePriceDiscovery()\n",
    "a_sale_price = AnnualSalePriceDiscovery()\n",
    "gnaf_dis = GnafPublicationDiscovery.create()\n",
    "\n",
    "gnaf_pub = gnaf_dis.get_publication()\n",
    "if gnaf_pub:\n",
    "    initialiser.add_target(gnaf_pub)\n",
    "\n",
    "lv_target = land_value.get_latest()\n",
    "if lv_target:\n",
    "    initialiser.add_target(lv_target)\n",
    "\n",
    "for sale_price_target in w_sale_price.get_links():\n",
    "    initialiser.add_target(sale_price_target)\n",
    "\n",
    "for sale_price_target in a_sale_price.get_links():\n",
    "    initialiser.add_target(sale_price_target)\n",
    "\n",
    "initialiser.setup_dirs()\n",
    "initialiser.fetch_remote_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd30133-e9d7-4ebb-b14b-ff3536338475",
   "metadata": {},
   "source": [
    "## Create Container with Database\n",
    "\n",
    "Here we are creating a container in docker from an image that uses the postgres image, which also installs a few extensions.\n",
    "\n",
    "### Note\n",
    "\n",
    "This notebook this is designed to be run more than once, so it'll throw away any existing container and database before creating a new one. After getting rid of any container using the same identifer, it'll create a new one and pull the relevant image if it's not already installed. It'll wait till the postgres instance is live then create the database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f709233-e243-468b-971e-df23bfcc0113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running ./zip-out/g-naf_aug24_allstates_gda2020_psv_1016/G-NAF/Extras/GNAF_TableCreation_Scripts/create_tables_ansi.sql\n",
      "running ./zip-out/g-naf_aug24_allstates_gda2020_psv_1016/G-NAF/Extras/GNAF_TableCreation_Scripts/add_fk_constraints.sql\n",
      "running sql/move_gnaf_to_schema.sql\n"
     ]
    }
   ],
   "source": [
    "from lib.gnaf_db import GnafDb, GnafContainer, GnafImage\n",
    "from lib import notebook_constants as nc\n",
    "\n",
    "image = GnafImage.create(tag=docker_image_tag)\n",
    "image.prepare()\n",
    "\n",
    "container = GnafContainer.create(container_name=docker_container_name, image=image)\n",
    "container.clean()\n",
    "container.prepare(db_conf, db_name)\n",
    "container.start()\n",
    "\n",
    "gnaf_db = GnafDb.create(db_conf, db_name)\n",
    "gnaf_db.wait_till_running()\n",
    "gnaf_db.init_schema(gnaf_pub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98949767-8727-4e9e-ad93-88ef38069e97",
   "metadata": {},
   "source": [
    "## Consume the ABS Shapefiles\n",
    "\n",
    "The [ABS provides a number of shape files][all abs shape files], we're going focus on 2 main sets of shapes. The **ABS Main Structures** which is stuff like SA1, 2, 3 & 4 along with greater cities, meshblocks, and states. As well as **Non ABS Main Structures** which is stuff like electoral divisions, suburbs post codes etc.\n",
    "\n",
    "[all abs shape files]: https://www.abs.gov.au/statistics/standards/australian-statistical-geography-standard-asgs-edition-3/jul2021-jun2026/access-and-downloads/digital-boundary-files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb443f14-e4d4-4ec6-ba21-c9e79e3b5e16",
   "metadata": {},
   "source": [
    "### ABS Main Structures \n",
    "\n",
    "Any address or region we look up in the GNAF dataset, we want to visualise. The ABS has a few different geographic groups which we can visualise the data against, but each address in the GNAF dataset has a meshblock id, which is the smaller block the ABS breaks addresses up into for SA1, SA2, SA3 and SA4's.\n",
    "\n",
    "This dataset is pretty useful for visualising the GNAF data for that reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76e9bba2-99c7-4db7-912f-4f1d817c3e8a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populated abs_main_structures.state with 10/10 rows.\n",
      "Populated abs_main_structures.gccsa with 35/35 rows.\n",
      "Populated abs_main_structures.sa4 with 108/108 rows.\n",
      "Populated abs_main_structures.sa3 with 359/359 rows.\n",
      "Populated abs_main_structures.sa2 with 2473/2473 rows.\n",
      "Populated abs_main_structures.sa1 with 61845/61845 rows.\n",
      "Populated abs_main_structures.meshblock with 368286/368286 rows.\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "    \n",
    "engine = gnaf_db.engine()\n",
    "\n",
    "schema = 'abs_main_structures'\n",
    "        \n",
    "column_renames_for_table = {\n",
    "    'SA1_2021_AUST_GDA2020': {\n",
    "        'SA1_CODE_2021': 'sa1_code', 'SA2_CODE_2021': 'sa2_code', 'SA3_CODE_2021': 'sa3_code',\n",
    "        'SA4_CODE_2021': 'sa4_code', 'GCCSA_CODE_2021': 'gcc_code', 'STATE_CODE_2021': 'state_code',\n",
    "        'AREA_ALBERS_SQKM': 'area_sqkm', 'geometry': 'geometry'\n",
    "    },\n",
    "    'SA2_2021_AUST_GDA2020': {\n",
    "        'SA2_CODE_2021': 'sa2_code', 'SA2_NAME_2021': 'sa2_name', 'SA3_CODE_2021': 'sa3_code',\n",
    "        'SA4_CODE_2021': 'sa4_code', 'GCCSA_CODE_2021': 'gcc_code', 'STATE_CODE_2021': 'state_code',\n",
    "        'AREA_ALBERS_SQKM': 'area_sqkm', 'geometry': 'geometry'\n",
    "    },\n",
    "    'SA3_2021_AUST_GDA2020': {\n",
    "        'SA3_CODE_2021': 'sa3_code', 'SA3_NAME_2021': 'sa3_name', 'SA4_CODE_2021': 'sa4_code',\n",
    "        'GCCSA_CODE_2021': 'gcc_code', 'STATE_CODE_2021': 'state_code', 'AREA_ALBERS_SQKM': 'area_sqkm',\n",
    "        'geometry': 'geometry'\n",
    "    },\n",
    "    'SA4_2021_AUST_GDA2020': {\n",
    "        'SA4_CODE_2021': 'sa4_code', 'SA4_NAME_2021': 'sa4_name', 'GCCSA_CODE_2021': 'gcc_code',\n",
    "        'STATE_CODE_2021': 'state_code', 'AREA_ALBERS_SQKM': 'area_sqkm', 'geometry': 'geometry'\n",
    "    },\n",
    "    'GCCSA_2021_AUST_GDA2020': {\n",
    "        'GCCSA_CODE_2021': 'gcc_code', 'GCCSA_NAME_2021': 'gcc_name', 'STATE_CODE_2021': 'state_code',\n",
    "        'geometry': 'geometry'\n",
    "    },\n",
    "    'STE_2021_AUST_GDA2020': {\n",
    "        'STATE_CODE_2021': 'state_code', 'STATE_NAME_2021': 'state_name', 'geometry': 'geometry'\n",
    "    },\n",
    "    'MB_2021_AUST_GDA2020': {\n",
    "        'MB_CODE_2021': 'mb_code', 'MB_CATEGORY_2021': 'mb_cat',\n",
    "        'SA1_CODE_2021': 'sa1_code', 'SA2_CODE_2021': 'sa2_code', 'SA3_CODE_2021': 'sa3_code',\n",
    "        'SA4_CODE_2021': 'sa4_code', 'GCCSA_CODE_2021': 'gcc_code', 'STATE_CODE_2021': 'state_code',\n",
    "        'AREA_ALBERS_SQKM': 'area_sqkm', 'geometry': 'geometry'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Column renames for each layer\n",
    "layers = {\n",
    "    'STE_2021_AUST_GDA2020': 'state',\n",
    "    'GCCSA_2021_AUST_GDA2020': 'gccsa',\n",
    "    'SA4_2021_AUST_GDA2020': 'sa4',\n",
    "    'SA3_2021_AUST_GDA2020': 'sa3',\n",
    "    'SA2_2021_AUST_GDA2020': 'sa2',\n",
    "    'SA1_2021_AUST_GDA2020': 'sa1',\n",
    "    'MB_2021_AUST_GDA2020': 'meshblock'\n",
    "}\n",
    "\n",
    "with gnaf_db.connect() as conn:\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # this really won't do anything unless you need to rerun this portion of the script\n",
    "    for _, table in layers.items():\n",
    "        cursor.execute(f\"\"\"\n",
    "        DO $$ BEGIN\n",
    "          IF EXISTS (\n",
    "            SELECT 1 FROM information_schema.tables \n",
    "             WHERE table_name = '{table}' AND table_schema = '{schema}'\n",
    "          ) THEN\n",
    "            TRUNCATE TABLE {schema}.{table} RESTART IDENTITY CASCADE;\n",
    "          END IF;\n",
    "        END $$;\n",
    "        \"\"\")\n",
    "    \n",
    "    with open('sql/abs_main_structures_create_tables.sql', 'r') as f:\n",
    "        cursor.execute(f.read())\n",
    "        \n",
    "    cursor.close()\n",
    "\n",
    "for layer_name, table_name in layers.items():\n",
    "    column_renames = column_renames_for_table[layer_name]\n",
    "    \n",
    "    # Load each layer into corresponding tables\n",
    "    df = gpd.read_file('zip-out/cities/ASGS_2021_MAIN_STRUCTURE_GDA2020.gpkg', layer=layer_name)\n",
    "    df = df.rename(columns=column_renames)\n",
    "    df = df[list(column_renames.values())]\n",
    "    df.to_postgis(table_name, engine, schema=schema, if_exists='append', index=False)\n",
    "\n",
    "    with engine.connect() as connection:\n",
    "        result = pd.read_sql(f\"SELECT COUNT(*) FROM {schema}.{table_name}\", connection)\n",
    "        print(f\"Populated {schema}.{table_name} with {result.iloc[0, 0]}/{len(df)} rows.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5485c6e-684d-4c6e-97f2-1745645d7eba",
   "metadata": {},
   "source": [
    "### Non Abs Main Structures \n",
    "\n",
    "We are mostly ingesting these to make it simpler to narrow data of interest. Typically if you're looking at this data, you're probably doing it some scope of relevance, such as a local government area, an electorate division, or whatever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "858db4bd-41b0-44df-8193-9da77825a02c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populated non_abs_main_structures.localities with 15353/15353 rows.\n",
      "Populated non_abs_main_structures.state_electoral_division_2021 with 452/452 rows.\n",
      "Populated non_abs_main_structures.state_electoral_division_2022 with 452/452 rows.\n",
      "Populated non_abs_main_structures.state_electoral_division_2024 with 452/452 rows.\n",
      "Populated non_abs_main_structures.federal_electoral_division_2021 with 170/170 rows.\n",
      "Populated non_abs_main_structures.lga_2021 with 566/566 rows.\n",
      "Populated non_abs_main_structures.lga_2022 with 566/566 rows.\n",
      "Populated non_abs_main_structures.lga_2023 with 566/566 rows.\n",
      "Populated non_abs_main_structures.lga_2024 with 566/566 rows.\n",
      "Populated non_abs_main_structures.post_code with 2644/2644 rows.\n",
      "Populated non_abs_main_structures.dzn with 9329/9329 rows.\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "schema = 'non_abs_main_structures'\n",
    "\n",
    "column_renames_for_table = {\n",
    "    'SAL_2021_AUST_GDA2020': {\n",
    "        \"SAL_CODE_2021\": \"locality_id\",\n",
    "        \"SAL_NAME_2021\": \"locality_name\",\n",
    "        \"STATE_CODE_2021\": \"state_code\",\n",
    "        \"AUS_CODE_2021\": \"in_australia\",\n",
    "        \"AREA_ALBERS_SQKM\": \"area_sqkm\",\n",
    "        \"geometry\": \"geometry\"\n",
    "    },\n",
    "    'SED_2021_AUST_GDA2020': {\n",
    "        \"SED_CODE_2021\": \"electorate_id\",\n",
    "        \"SED_NAME_2021\": \"electorate_name\",\n",
    "        \"STATE_CODE_2021\": \"state_code\",\n",
    "        \"AUS_CODE_2021\": \"in_australia\",\n",
    "        \"AREA_ALBERS_SQKM\": \"area_sqkm\",\n",
    "        \"geometry\": \"geometry\"\n",
    "    },\n",
    "    'SED_2022_AUST_GDA2020': {\n",
    "        \"SED_CODE_2022\": \"electorate_id\",\n",
    "        \"SED_NAME_2022\": \"electorate_name\",\n",
    "        \"STATE_CODE_2021\": \"state_code\",\n",
    "        \"AUS_CODE_2021\": \"in_australia\",\n",
    "        \"AREA_ALBERS_SQKM\": \"area_sqkm\",\n",
    "        \"geometry\": \"geometry\"\n",
    "    },\n",
    "    'SED_2024_AUST_GDA2020': {\n",
    "        \"SED_CODE_2024\": \"electorate_id\",\n",
    "        \"SED_NAME_2024\": \"electorate_name\",\n",
    "        \"STATE_CODE_2021\": \"state_code\",\n",
    "        \"AUS_CODE_2021\": \"in_australia\",\n",
    "        \"AREA_ALBERS_SQKM\": \"area_sqkm\",\n",
    "        \"geometry\": \"geometry\"\n",
    "    },\n",
    "    'CED_2021_AUST_GDA2020': {\n",
    "        \"CED_CODE_2021\": \"electorate_id\",\n",
    "        \"CED_NAME_2021\": \"electorate_name\",\n",
    "        \"STATE_CODE_2021\": \"state_code\",\n",
    "        \"AUS_CODE_2021\": \"in_australia\",\n",
    "        \"AREA_ALBERS_SQKM\": \"area_sqkm\",\n",
    "        \"geometry\": \"geometry\"\n",
    "    },\n",
    "    'LGA_2021_AUST_GDA2020': {\n",
    "        \"LGA_CODE_2021\": \"lga_id\",\n",
    "        \"LGA_NAME_2021\": \"lga_name\",\n",
    "        \"STATE_CODE_2021\": \"state_code\",\n",
    "        \"AUS_CODE_2021\": \"in_australia\",\n",
    "        \"AREA_ALBERS_SQKM\": \"area_sqkm\",\n",
    "        \"geometry\": \"geometry\"\n",
    "    },\n",
    "    'LGA_2022_AUST_GDA2020': {\n",
    "        \"LGA_CODE_2022\": \"lga_id\",\n",
    "        \"LGA_NAME_2022\": \"lga_name\",\n",
    "        \"STATE_CODE_2021\": \"state_code\",\n",
    "        \"AUS_CODE_2021\": \"in_australia\",\n",
    "        \"AREA_ALBERS_SQKM\": \"area_sqkm\",\n",
    "        \"geometry\": \"geometry\"\n",
    "    },\n",
    "    'LGA_2023_AUST_GDA2020': {\n",
    "        \"LGA_CODE_2023\": \"lga_id\",\n",
    "        \"LGA_NAME_2023\": \"lga_name\",\n",
    "        \"STATE_CODE_2021\": \"state_code\",\n",
    "        \"AUS_CODE_2021\": \"in_australia\",\n",
    "        \"AREA_ALBERS_SQKM\": \"area_sqkm\",\n",
    "        \"geometry\": \"geometry\"\n",
    "    },\n",
    "    'LGA_2024_AUST_GDA2020': {\n",
    "        \"LGA_CODE_2024\": \"lga_id\",\n",
    "        \"LGA_NAME_2024\": \"lga_name\",\n",
    "        \"STATE_CODE_2021\": \"state_code\",\n",
    "        \"AUS_CODE_2021\": \"in_australia\",\n",
    "        \"AREA_ALBERS_SQKM\": \"area_sqkm\",\n",
    "        \"geometry\": \"geometry\"\n",
    "    },\n",
    "    'POA_2021_AUST_GDA2020': {\n",
    "        \"POA_CODE_2021\": \"post_code\",\n",
    "        \"AUS_CODE_2021\": \"in_australia\",\n",
    "        \"AREA_ALBERS_SQKM\": \"area_sqkm\",\n",
    "        \"geometry\": \"geometry\"\n",
    "    },\n",
    "    # https://www.abs.gov.au/statistics/standards/australian-statistical-geography-standard-asgs-edition-3/jul2021-jun2026/non-abs-structures/destination-zones\n",
    "    'DZN_2021_AUST_GDA2020': {\n",
    "        'DZN_CODE_2021': 'dzn_code', \n",
    "        'SA2_CODE_2021': 'sa2_code',\n",
    "        'STATE_CODE_2021': 'state_code',\n",
    "        \"AUS_CODE_2021\": 'in_australia',\n",
    "        \"AREA_ALBERS_SQKM\": \"area_sqkm\",\n",
    "        \"geometry\": \"geometry\",\n",
    "    },\n",
    "}\n",
    "\n",
    "layers = {\n",
    "    'SAL_2021_AUST_GDA2020': 'localities',\n",
    "    'SED_2021_AUST_GDA2020': 'state_electoral_division_2021',\n",
    "    'SED_2022_AUST_GDA2020': 'state_electoral_division_2022',\n",
    "    'SED_2024_AUST_GDA2020': 'state_electoral_division_2024',\n",
    "    'CED_2021_AUST_GDA2020': 'federal_electoral_division_2021',\n",
    "    'LGA_2021_AUST_GDA2020': 'lga_2021',\n",
    "    'LGA_2022_AUST_GDA2020': 'lga_2022',\n",
    "    'LGA_2023_AUST_GDA2020': 'lga_2023',\n",
    "    'LGA_2024_AUST_GDA2020': 'lga_2024',\n",
    "    'POA_2021_AUST_GDA2020': 'post_code',\n",
    "    'DZN_2021_AUST_GDA2020': 'dzn',\n",
    "    # Unused\n",
    "    # - australian drainage divisions, 'ADD_2021_AUST_GDA2020'\n",
    "    # - tourism regions, 'TR_2021_AUST_GDA2020'\n",
    "}\n",
    "\n",
    "with gnaf_db.connect() as conn:\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # this really won't do anything unless you need to rerun this portion of the script\n",
    "    for _, table in layers.items():\n",
    "        cursor.execute(f\"\"\"\n",
    "        DO $$ BEGIN\n",
    "          IF EXISTS (\n",
    "            SELECT 1 FROM information_schema.tables \n",
    "             WHERE table_name = '{table}' AND table_schema = '{schema}'\n",
    "          ) THEN\n",
    "            TRUNCATE TABLE {schema}.{table} RESTART IDENTITY CASCADE;\n",
    "          END IF;\n",
    "        END $$;\n",
    "        \"\"\")\n",
    "    \n",
    "    with open('sql/non_abs_main_structures_create_tables.sql', 'r') as f:\n",
    "        cursor.execute(f.read())\n",
    "        \n",
    "    cursor.close()\n",
    "    \n",
    "for layer_name, table_name in layers.items():\n",
    "    column_renames = column_renames_for_table[layer_name]\n",
    "    \n",
    "    df = gpd.read_file('zip-out/non_abs_structures_shapefiles/ASGS_Ed3_Non_ABS_Structures_GDA2020_updated_2024.gpkg', layer=layer_name)\n",
    "    df = df.rename(columns=column_renames)\n",
    "    df = df[list(column_renames.values())]\n",
    "\n",
    "    if 'in_australia' in df:\n",
    "        df['in_australia'] = df['in_australia'] == 'AUS'\n",
    "    \n",
    "    df.to_postgis(table_name, engine, schema=schema, if_exists='append', index=False)\n",
    "\n",
    "    with engine.connect() as connection:\n",
    "        result = pd.read_sql(f\"SELECT COUNT(*) FROM {schema}.{table_name}\", connection)\n",
    "        print(f\"Populated {schema}.{table_name} with {result.iloc[0, 0]}/{len(df)} rows.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd88dacf-a7dc-4fc2-b0b7-c8c78c5d90ff",
   "metadata": {},
   "source": [
    "## Ingesting NSW Land Values\n",
    "\n",
    "First lets just get the CSV's into the database, then we'll break it up into seperates tables, then we'll form links with the GNAF dataset.\n",
    "\n",
    "#### Documentation on this dataset\n",
    "\n",
    "The valuer general website has a link to documentation on interpretting that data on [this page](https://www.nsw.gov.au/housing-and-construction/land-values-nsw/resource-library/land-value-information-user-guide). I didn't link to the PDF directly as it occasionally updated and a direct link is at risk of going stale.\n",
    "\n",
    "It's useful getting the meaning behind the codes and terms used in the bulk data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d53250-00db-4582-a6de-3f15be72d14a",
   "metadata": {},
   "source": [
    "### Build the `nsw_valuer_general.raw_entries_lv` table\n",
    "\n",
    "Here we are just loading the each file from the latest land value publication with minimal changes, and a bit of sanitisizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9231a7c-ed31-4016-8e30-d052ad83294f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-08 19:56:21 Consumed zip-out/nswvg_lv_01_Sep_2024/052_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 4885\n",
      "2024-09-08 19:56:21 Consumed zip-out/nswvg_lv_01_Sep_2024/043_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 8034\n",
      "2024-09-08 19:56:21 Consumed zip-out/nswvg_lv_01_Sep_2024/054_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 11236\n",
      "2024-09-08 19:56:21 Consumed zip-out/nswvg_lv_01_Sep_2024/061_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 11236\n",
      "2024-09-08 19:56:21 Consumed zip-out/nswvg_lv_01_Sep_2024/051_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 16450\n",
      "2024-09-08 19:56:22 Consumed zip-out/nswvg_lv_01_Sep_2024/065_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 244312024-09-08 19:56:22 Consumed zip-out/nswvg_lv_01_Sep_2024/066_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 24431\n",
      "\n",
      "2024-09-08 19:56:22 Consumed zip-out/nswvg_lv_01_Sep_2024/070_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 28257\n",
      "2024-09-08 19:56:23 Consumed zip-out/nswvg_lv_01_Sep_2024/083_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 31977\n",
      "2024-09-08 19:56:26 Consumed zip-out/nswvg_lv_01_Sep_2024/007_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 39855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l1/1wl5vmds75qfv945_bs13x6w0000gn/T/ipykernel_15187/2975886609.py:31: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(full_file_path, encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-08 19:56:28 Consumed zip-out/nswvg_lv_01_Sep_2024/002_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 45287\n",
      "2024-09-08 19:56:30 Consumed zip-out/nswvg_lv_01_Sep_2024/012_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 56349\n",
      "2024-09-08 19:56:31 Consumed zip-out/nswvg_lv_01_Sep_2024/018_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 75832\n",
      "2024-09-08 19:56:31 Consumed zip-out/nswvg_lv_01_Sep_2024/085_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 83621\n",
      "2024-09-08 19:56:33 Consumed zip-out/nswvg_lv_01_Sep_2024/087_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 90528\n",
      "2024-09-08 19:56:33 Consumed zip-out/nswvg_lv_01_Sep_2024/042_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 98073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l1/1wl5vmds75qfv945_bs13x6w0000gn/T/ipykernel_15187/2975886609.py:31: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(full_file_path, encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-08 19:56:39 Consumed zip-out/nswvg_lv_01_Sep_2024/008_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 157081\n",
      "2024-09-08 19:56:41 Consumed zip-out/nswvg_lv_01_Sep_2024/001_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 213056\n",
      "2024-09-08 19:56:41 Consumed zip-out/nswvg_lv_01_Sep_2024/090_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 213056\n",
      "2024-09-08 19:56:42 Consumed zip-out/nswvg_lv_01_Sep_2024/050_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 237852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l1/1wl5vmds75qfv945_bs13x6w0000gn/T/ipykernel_15187/2975886609.py:31: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(full_file_path, encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-08 19:56:45 Consumed zip-out/nswvg_lv_01_Sep_2024/074_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 258957\n",
      "2024-09-08 19:56:45 Consumed zip-out/nswvg_lv_01_Sep_2024/098_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 258957\n",
      "2024-09-08 19:56:46 Consumed zip-out/nswvg_lv_01_Sep_2024/088_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 269298\n",
      "2024-09-08 19:56:47 Consumed zip-out/nswvg_lv_01_Sep_2024/082_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 310782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l1/1wl5vmds75qfv945_bs13x6w0000gn/T/ipykernel_15187/2975886609.py:31: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(full_file_path, encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-08 19:56:50 Consumed zip-out/nswvg_lv_01_Sep_2024/109_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 318226\n",
      "2024-09-08 19:56:51 Consumed zip-out/nswvg_lv_01_Sep_2024/092_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 335574\n",
      "2024-09-08 19:56:53 Consumed zip-out/nswvg_lv_01_Sep_2024/117_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 345446\n",
      "2024-09-08 19:56:54 Consumed zip-out/nswvg_lv_01_Sep_2024/118_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 349380\n",
      "2024-09-08 19:56:54 Consumed zip-out/nswvg_lv_01_Sep_2024/123_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 349380\n",
      "2024-09-08 19:56:56 Consumed zip-out/nswvg_lv_01_Sep_2024/116_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 357841\n",
      "2024-09-08 19:56:59 Consumed zip-out/nswvg_lv_01_Sep_2024/010_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 389550\n",
      "2024-09-08 19:57:01 Consumed zip-out/nswvg_lv_01_Sep_2024/097_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 448270\n",
      "2024-09-08 19:57:02 Consumed zip-out/nswvg_lv_01_Sep_2024/084_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 448270\n",
      "2024-09-08 19:57:03 Consumed zip-out/nswvg_lv_01_Sep_2024/137_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 455989\n",
      "2024-09-08 19:57:03 Consumed zip-out/nswvg_lv_01_Sep_2024/081_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 518911\n",
      "2024-09-08 19:57:05 Consumed zip-out/nswvg_lv_01_Sep_2024/100_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 547607\n",
      "2024-09-08 19:57:06 Consumed zip-out/nswvg_lv_01_Sep_2024/143_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 579827\n",
      "2024-09-08 19:57:06 Consumed zip-out/nswvg_lv_01_Sep_2024/102_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 579827\n",
      "2024-09-08 19:57:08 Consumed zip-out/nswvg_lv_01_Sep_2024/005_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 617457\n",
      "2024-09-08 19:57:12 Consumed zip-out/nswvg_lv_01_Sep_2024/149_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 623831\n",
      "2024-09-08 19:57:15 Consumed zip-out/nswvg_lv_01_Sep_2024/148_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 656814\n",
      "2024-09-08 19:57:15 Consumed zip-out/nswvg_lv_01_Sep_2024/158_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 656814\n",
      "2024-09-08 19:57:16 Consumed zip-out/nswvg_lv_01_Sep_2024/151_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 694365\n",
      "2024-09-08 19:57:16 Consumed zip-out/nswvg_lv_01_Sep_2024/139_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 694365\n",
      "2024-09-08 19:57:17 Consumed zip-out/nswvg_lv_01_Sep_2024/124_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 694365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l1/1wl5vmds75qfv945_bs13x6w0000gn/T/ipykernel_15187/2975886609.py:31: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(full_file_path, encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-08 19:57:17 Consumed zip-out/nswvg_lv_01_Sep_2024/150_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 708307\n",
      "2024-09-08 19:57:19 Consumed zip-out/nswvg_lv_01_Sep_2024/157_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 723444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l1/1wl5vmds75qfv945_bs13x6w0000gn/T/ipykernel_15187/2975886609.py:31: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(full_file_path, encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-08 19:57:22 Consumed zip-out/nswvg_lv_01_Sep_2024/164_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 739914\n",
      "2024-09-08 19:57:23 Consumed zip-out/nswvg_lv_01_Sep_2024/187_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 739914\n",
      "2024-09-08 19:57:23 Consumed zip-out/nswvg_lv_01_Sep_2024/199_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 743113\n",
      "2024-09-08 19:57:28 Consumed zip-out/nswvg_lv_01_Sep_2024/004_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 8961692024-09-08 19:57:28 Consumed zip-out/nswvg_lv_01_Sep_2024/192_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 896169\n",
      "\n",
      "2024-09-08 19:57:28 Consumed zip-out/nswvg_lv_01_Sep_2024/101_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 896169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l1/1wl5vmds75qfv945_bs13x6w0000gn/T/ipykernel_15187/2975886609.py:31: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(full_file_path, encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-08 19:57:30 Consumed zip-out/nswvg_lv_01_Sep_2024/159_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 924013\n",
      "2024-09-08 19:57:30 Consumed zip-out/nswvg_lv_01_Sep_2024/188_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 924013\n",
      "2024-09-08 19:57:31 Consumed zip-out/nswvg_lv_01_Sep_2024/209_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 937161\n",
      "2024-09-08 19:57:34 Consumed zip-out/nswvg_lv_01_Sep_2024/152_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 9796382024-09-08 19:57:34 Consumed zip-out/nswvg_lv_01_Sep_2024/210_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 979638\n",
      "\n",
      "2024-09-08 19:57:38 Consumed zip-out/nswvg_lv_01_Sep_2024/103_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1050686\n",
      "2024-09-08 19:57:39 Consumed zip-out/nswvg_lv_01_Sep_2024/230_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1052331\n",
      "2024-09-08 19:57:40 Consumed zip-out/nswvg_lv_01_Sep_2024/231_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1068892\n",
      "2024-09-08 19:57:40 Consumed zip-out/nswvg_lv_01_Sep_2024/222_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1100800\n",
      "2024-09-08 19:57:40 Consumed zip-out/nswvg_lv_01_Sep_2024/171_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1100800\n",
      "2024-09-08 19:57:41 Consumed zip-out/nswvg_lv_01_Sep_2024/207_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1127427\n",
      "2024-09-08 19:57:41 Consumed zip-out/nswvg_lv_01_Sep_2024/232_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1130742\n",
      "2024-09-08 19:57:42 Consumed zip-out/nswvg_lv_01_Sep_2024/233_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1130742\n",
      "2024-09-08 19:57:43 Consumed zip-out/nswvg_lv_01_Sep_2024/235_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1132621\n",
      "2024-09-08 19:57:44 Consumed zip-out/nswvg_lv_01_Sep_2024/144_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1198090\n",
      "2024-09-08 19:57:44 Consumed zip-out/nswvg_lv_01_Sep_2024/236_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1198090\n",
      "2024-09-08 19:57:47 Consumed zip-out/nswvg_lv_01_Sep_2024/238_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1200828\n",
      "2024-09-08 19:57:47 Consumed zip-out/nswvg_lv_01_Sep_2024/234_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1213424\n",
      "2024-09-08 19:57:48 Consumed zip-out/nswvg_lv_01_Sep_2024/239_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1213424\n",
      "2024-09-08 19:57:49 Consumed zip-out/nswvg_lv_01_Sep_2024/243_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1217949\n",
      "2024-09-08 19:57:50 Consumed zip-out/nswvg_lv_01_Sep_2024/240_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1217949\n",
      "2024-09-08 19:57:50 Consumed zip-out/nswvg_lv_01_Sep_2024/219_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1266465\n",
      "2024-09-08 19:57:50 Consumed zip-out/nswvg_lv_01_Sep_2024/226_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1266465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l1/1wl5vmds75qfv945_bs13x6w0000gn/T/ipykernel_15187/2975886609.py:31: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(full_file_path, encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-08 19:57:53 Consumed zip-out/nswvg_lv_01_Sep_2024/244_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1308208\n",
      "2024-09-08 19:57:54 Consumed zip-out/nswvg_lv_01_Sep_2024/216_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1320891\n",
      "2024-09-08 19:57:54 Consumed zip-out/nswvg_lv_01_Sep_2024/251_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1320891\n",
      "2024-09-08 19:57:54 Consumed zip-out/nswvg_lv_01_Sep_2024/247_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1320891\n",
      "2024-09-08 19:57:54 Consumed zip-out/nswvg_lv_01_Sep_2024/252_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1328523\n",
      "2024-09-08 19:57:55 Consumed zip-out/nswvg_lv_01_Sep_2024/254_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1328523\n",
      "2024-09-08 19:57:55 Consumed zip-out/nswvg_lv_01_Sep_2024/250_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1328523\n",
      "2024-09-08 19:57:57 Consumed zip-out/nswvg_lv_01_Sep_2024/217_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1379818\n",
      "2024-09-08 19:57:57 Consumed zip-out/nswvg_lv_01_Sep_2024/253_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1384406\n",
      "2024-09-08 19:57:57 Consumed zip-out/nswvg_lv_01_Sep_2024/255_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1384406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l1/1wl5vmds75qfv945_bs13x6w0000gn/T/ipykernel_15187/2975886609.py:31: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(full_file_path, encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-08 19:58:01 Consumed zip-out/nswvg_lv_01_Sep_2024/262_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1389636\n",
      "2024-09-08 19:58:02 Consumed zip-out/nswvg_lv_01_Sep_2024/257_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1402825\n",
      "2024-09-08 19:58:03 Consumed zip-out/nswvg_lv_01_Sep_2024/263_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1410736\n",
      "2024-09-08 19:58:04 Consumed zip-out/nswvg_lv_01_Sep_2024/218_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1468660\n",
      "2024-09-08 19:58:06 Consumed zip-out/nswvg_lv_01_Sep_2024/265_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1475403\n",
      "2024-09-08 19:58:07 Consumed zip-out/nswvg_lv_01_Sep_2024/266_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1487180\n",
      "2024-09-08 19:58:08 Consumed zip-out/nswvg_lv_01_Sep_2024/220_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1541238\n",
      "2024-09-08 19:58:09 Consumed zip-out/nswvg_lv_01_Sep_2024/270_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1543834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l1/1wl5vmds75qfv945_bs13x6w0000gn/T/ipykernel_15187/2975886609.py:31: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(full_file_path, encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-08 19:58:10 Consumed zip-out/nswvg_lv_01_Sep_2024/269_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1552745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l1/1wl5vmds75qfv945_bs13x6w0000gn/T/ipykernel_15187/2975886609.py:31: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(full_file_path, encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-08 19:58:16 Consumed zip-out/nswvg_lv_01_Sep_2024/274_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1562106\n",
      "2024-09-08 19:58:17 Consumed zip-out/nswvg_lv_01_Sep_2024/273_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1643859\n",
      "2024-09-08 19:58:18 Consumed zip-out/nswvg_lv_01_Sep_2024/224_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1643859\n",
      "2024-09-08 19:58:20 Consumed zip-out/nswvg_lv_01_Sep_2024/223_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1711459\n",
      "2024-09-08 19:58:24 Consumed zip-out/nswvg_lv_01_Sep_2024/300_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1714789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l1/1wl5vmds75qfv945_bs13x6w0000gn/T/ipykernel_15187/2975886609.py:31: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(full_file_path, encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-08 19:58:25 Consumed zip-out/nswvg_lv_01_Sep_2024/301_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1755276\n",
      "2024-09-08 19:58:26 Consumed zip-out/nswvg_lv_01_Sep_2024/264_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1755276\n",
      "2024-09-08 19:58:29 Consumed zip-out/nswvg_lv_01_Sep_2024/302_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1760742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l1/1wl5vmds75qfv945_bs13x6w0000gn/T/ipykernel_15187/2975886609.py:31: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(full_file_path, encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-08 19:58:35 Consumed zip-out/nswvg_lv_01_Sep_2024/272_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 18354252024-09-08 19:58:35 Consumed zip-out/nswvg_lv_01_Sep_2024/260_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1835425\n",
      "\n",
      "2024-09-08 19:58:38 Consumed zip-out/nswvg_lv_01_Sep_2024/275_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1868357\n",
      "2024-09-08 19:58:39 Consumed zip-out/nswvg_lv_01_Sep_2024/511_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1876808\n",
      "2024-09-08 19:58:39 Consumed zip-out/nswvg_lv_01_Sep_2024/528_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1883804\n",
      "2024-09-08 19:58:40 Consumed zip-out/nswvg_lv_01_Sep_2024/526_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1883804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l1/1wl5vmds75qfv945_bs13x6w0000gn/T/ipykernel_15187/2975886609.py:31: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(full_file_path, encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-08 19:58:41 Consumed zip-out/nswvg_lv_01_Sep_2024/261_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1935100\n",
      "2024-09-08 19:58:44 Consumed zip-out/nswvg_lv_01_Sep_2024/267_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 1986619\n",
      "2024-09-08 19:58:45 Consumed zip-out/nswvg_lv_01_Sep_2024/538_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 20299152024-09-08 19:58:45 Consumed zip-out/nswvg_lv_01_Sep_2024/537_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 2029915\n",
      "\n",
      "2024-09-08 19:58:45 Consumed zip-out/nswvg_lv_01_Sep_2024/276_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 2029915\n",
      "2024-09-08 19:58:50 Consumed zip-out/nswvg_lv_01_Sep_2024/560_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 2037088\n",
      "2024-09-08 19:58:52 Consumed zip-out/nswvg_lv_01_Sep_2024/529_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 2053665\n",
      "2024-09-08 19:58:54 Consumed zip-out/nswvg_lv_01_Sep_2024/214_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 2178410\n",
      "2024-09-08 19:58:56 Consumed zip-out/nswvg_lv_01_Sep_2024/620_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 2246889\n",
      "2024-09-08 19:58:56 Consumed zip-out/nswvg_lv_01_Sep_2024/268_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 2273938\n",
      "2024-09-08 19:58:57 Consumed zip-out/nswvg_lv_01_Sep_2024/303_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 2273938\n",
      "2024-09-08 19:59:01 Consumed zip-out/nswvg_lv_01_Sep_2024/608_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 2294001\n",
      "2024-09-08 19:59:06 Consumed zip-out/nswvg_lv_01_Sep_2024/258_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 2383522\n",
      "2024-09-08 19:59:09 Consumed zip-out/nswvg_lv_01_Sep_2024/271_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 2481461\n",
      "2024-09-08 19:59:09 Consumed zip-out/nswvg_lv_01_Sep_2024/575_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 2481461\n",
      "2024-09-08 19:59:12 Consumed zip-out/nswvg_lv_01_Sep_2024/708_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 2538920\n",
      "2024-09-08 19:59:12 Consumed zip-out/nswvg_lv_01_Sep_2024/666_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 2538920\n",
      "2024-09-08 19:59:13 Consumed zip-out/nswvg_lv_01_Sep_2024/656_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 2573729\n",
      "2024-09-08 19:59:18 Consumed zip-out/nswvg_lv_01_Sep_2024/259_LAND_VALUE_DATA_20240901.csv, raw_entries_lv 2702450\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sqlalchemy import text\n",
    "from psycopg2.errors import StringDataRightTruncation\n",
    "\n",
    "from lib import notebook_constants as nc\n",
    "\n",
    "with gnaf_db.connect() as conn:\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS nsw_valuer_general.raw_entries_lv CASCADE\")\n",
    "    with open('sql/nsw_lv_schema_1_raw.sql', 'r') as f:\n",
    "        cursor.execute(f.read())\n",
    "    cursor.close()\n",
    "            \n",
    "column_mappings = { **nc.lv_long_column_mappings, **nc.lv_wide_columns_mappings }\n",
    "\n",
    "def count(table, source = None):\n",
    "    c = pd.read_sql(f'SELECT count(*) FROM nsw_valuer_general.{table}', gnaf_db.engine())\n",
    "    time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f'{time} {source and f\"{source}, \" or \"\"}{table} {c.iloc[0,0]}')\n",
    "\n",
    "def process_file(file):\n",
    "    if not file.endswith(\"csv\"):\n",
    "        return\n",
    "\n",
    "    full_file_path = f\"zip-out/{lv_target.zip_dst}/{file}\"\n",
    "    try:\n",
    "        df = pd.read_csv(full_file_path, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        # Fallback to ISO-8859-1 encoding if utf-8 fails\n",
    "        df = pd.read_csv(full_file_path, encoding='ISO-8859-1')\n",
    "\n",
    "    date_str = file.split('_')[-1].replace('.csv', '')\n",
    "    \n",
    "    df.index.name = 'source_file_position'\n",
    "    df = df.drop(columns=['Unnamed: 34'])\n",
    "    df = df.rename(columns=column_mappings).reset_index()\n",
    "    df['source_file_name'] = file\n",
    "    df['source_date'] = datetime.strptime(date_str, \"%Y%m%d\")\n",
    "    df['postcode'] = [(n if math.isnan(n) else str(int(n))) for n in df['postcode']]\n",
    "    \n",
    "    try:\n",
    "        df.to_sql('raw_entries_lv', gnaf_db.engine(), schema='nsw_valuer_general', if_exists='append', index=False)\n",
    "    finally:\n",
    "        count('raw_entries_lv', f'Consumed {full_file_path}')\n",
    "\n",
    "files = sorted(os.listdir(f\"zip-out/{lv_target.zip_dst}\"))\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "    futures = [executor.submit(process_file, file) for file in files]\n",
    "    for future in as_completed(futures):\n",
    "        future.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b508ff7-bc8e-42fd-ae04-9c17ca41277e",
   "metadata": {},
   "source": [
    "### Break CSV data into sepreate relations\n",
    "\n",
    "Just to break up the data into more efficent representations of the data, and data that will be easier to query, we're going to perform a series of queries against the GNAF data before using it populate the tables we care about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7fd41df-6ac9-4af7-bf21-2efe785c4313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-08 20:01:01 district 128\n",
      "2024-09-08 20:01:01 suburb 5075\n",
      "2024-09-08 20:01:01 street 128422\n",
      "2024-09-08 20:01:01 property 2702450\n",
      "2024-09-08 20:01:01 property_description 2702450\n",
      "2024-09-08 20:01:02 valuations 13512250\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sqlalchemy import text\n",
    "from psycopg2.errors import StringDataRightTruncation\n",
    "\n",
    "with gnaf_db.connect() as conn:\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS nsw_valuer_general.source_file CASCADE\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS nsw_valuer_general.source CASCADE\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS nsw_valuer_general.district CASCADE\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS nsw_valuer_general.suburb CASCADE\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS nsw_valuer_general.street CASCADE\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS nsw_valuer_general.property CASCADE\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS nsw_valuer_general.property_description CASCADE\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS nsw_valuer_general.valuations CASCADE\")\n",
    "    \n",
    "    with open('sql/nsw_lv_schema_2_structure.sql', 'r') as f:\n",
    "        cursor.execute(f.read())\n",
    "        \n",
    "    with open('sql/nsw_lv_from_raw.sql', 'r') as f:\n",
    "        cursor.execute(f.read())\n",
    "        \n",
    "    cursor.close()\n",
    "    \n",
    "count('district')\n",
    "count('suburb')\n",
    "count('street')\n",
    "count('property')\n",
    "count('property_description')\n",
    "count('valuations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccc4af0-cb98-4ee7-bd40-10cca53f7153",
   "metadata": {},
   "source": [
    "### Parse contents of the property description\n",
    "\n",
    "The `property_description` from the original valuer general data constains alot of information. The most important of which is the land parcel or `lot/plan` information. There is other information in there as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2bc00d3-460d-4d95-8da5-e36d6b7b0053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table nsw_valuer_general.property has 2702450 rows\n",
      "Table nsw_valuer_general.land_parcel_link has 4245674 rows\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lib.nsw_vg.property_description import parse_land_parcel_ids\n",
    "\n",
    "engine = gnaf_db.engine()\n",
    "\n",
    "with gnaf_db.connect() as conn:\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS nsw_valuer_general.land_parcel_link\")\n",
    "    with open('sql/nsw_lv_schema_3_property_description_meta_data.sql', 'r') as f:\n",
    "        cursor.execute(f.read())\n",
    "    cursor.close()\n",
    "\n",
    "def land_parcels(desc):\n",
    "    desc, parcels = parse_land_parcel_ids(desc)\n",
    "    return parcels \n",
    "\n",
    "query = \"SELECT * FROM nsw_valuer_general.property_description\"\n",
    "for df_chunk in pd.read_sql(query, engine, chunksize=10000):\n",
    "    df_chunk = df_chunk.dropna(subset=['property_description'])\n",
    "    df_chunk['parcels'] = df_chunk['property_description'].apply(land_parcels)\n",
    "    df_chunk_ex = df_chunk.explode('parcels')\n",
    "    df_chunk_ex = df_chunk_ex.dropna(subset=['parcels'])\n",
    "    df_chunk_ex['land_parcel_id'] = df_chunk_ex['parcels'].apply(lambda p: p.id)\n",
    "    df_chunk_ex['part'] = df_chunk_ex['parcels'].apply(lambda p: p.part)\n",
    "    df_chunk_ex = df_chunk_ex.drop(columns=['property_description', 'parcels'])\n",
    "    df_chunk_ex.to_sql(\n",
    "        'land_parcel_link',\n",
    "        con=engine,\n",
    "        schema='nsw_valuer_general',\n",
    "        if_exists='append',\n",
    "        index=False,\n",
    "    )\n",
    "\n",
    "with gnaf_db.connect() as conn:\n",
    "    cursor = conn.cursor()\n",
    "    for t in ['property', 'land_parcel_link']:\n",
    "        cursor.execute(f'SELECT COUNT(*) FROM nsw_valuer_general.{t}')\n",
    "        count = cursor.fetchone()[0]\n",
    "        print(f\"Table nsw_valuer_general.{t} has {count} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0851f3-8b55-4751-894e-408693dda480",
   "metadata": {},
   "source": [
    "### Get rid of `raw_entries_lv` table\n",
    "\n",
    "We no longer need the raw entries table, deleting it should make the database a bit efficent in terms of storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b86be0dd-c5a9-4516-8509-bd10c8a76ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping raw entries table\n"
     ]
    }
   ],
   "source": [
    "with gnaf_db.connect() as conn:\n",
    "    cursor = conn.cursor()\n",
    "    if GLOBAL_FLAGS['drop_raw_nsw_valuer_general_entries']:\n",
    "        cursor.execute(\"DROP TABLE IF EXISTS nsw_valuer_general.raw_entries_lv\")\n",
    "        print(\"Dropping raw entries table\")\n",
    "    else:\n",
    "        print(\"Keeping raw entries table\")\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f88b7d1-1a2c-430e-8536-2635a1002928",
   "metadata": {},
   "source": [
    "## Ingest NSW Sales data\n",
    "\n",
    "This data is also from the NSW valuer general\n",
    "\n",
    "#### Documentation on this dataset\n",
    "\n",
    "You can find that [here](https://www.nsw.gov.au/housing-and-construction/land-values-nsw/resource-library/property-sales-data-guide)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6b5e4a-2d8b-41da-8d4c-ed464951f2fb",
   "metadata": {},
   "source": [
    "### Build the `nsw_valuer_general.raw_entries_ps` table\n",
    "\n",
    "First lets populate the raw sales information into the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1094aace-e428-41bc-9caa-15e5650e7d05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "862bdf7e-1573-47e6-a5c4-7252ddb546b6",
   "metadata": {},
   "source": [
    "## Gnaf Ingestion\n",
    "\n",
    "The main thing to consider with ingesting this data is the order in which it is ingested. Now you could actually add the foreign key constraints after populating the database, and go nuts (That might actually even be faster than what I got here). But after a day of different variants of this script while trying to juggle correctness of data ingested and speed, I'm settling for this.\n",
    "\n",
    "So 90% of the code here is just coordinating the dependencies and order in which everything is ingested, as well as doing as much in parallel as possible. My earlier approach of doing everything sequentially took 6 hours, is between 1-2 hours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37cc6440-562d-4468-b5d9-d3d53c925ae6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-08 20:02:13 Populating from Authority_Code_ADDRESS_CHANGE_TYPE_AUT_psv.psv\n",
      "2024-09-08 20:02:13 Populating from Authority_Code_FLAT_TYPE_AUT_psv.psv\n",
      "2024-09-08 20:02:13 Populating from Authority_Code_GEOCODE_TYPE_AUT_psv.psv\n",
      "2024-09-08 20:02:13 Populating from Authority_Code_GEOCODE_RELIABILITY_AUT_psv.psv\n",
      "2024-09-08 20:02:13 Populating from Authority_Code_STREET_CLASS_AUT_psv.psv\n",
      "2024-09-08 20:02:13 Populating from Authority_Code_GEOCODED_LEVEL_TYPE_AUT_psv.psv\n",
      "2024-09-08 20:02:13 Populating from Authority_Code_LOCALITY_CLASS_AUT_psv.psv\n",
      "2024-09-08 20:02:13 Populating from Authority_Code_PS_JOIN_TYPE_AUT_psv.psv\n",
      "2024-09-08 20:02:13 Populating from Authority_Code_MB_MATCH_CODE_AUT_psv.psv\n",
      "2024-09-08 20:02:13 Populating from Authority_Code_ADDRESS_ALIAS_TYPE_AUT_psv.psv\n",
      "2024-09-08 20:02:13 Populating from Authority_Code_STREET_SUFFIX_AUT_psv.psv\n",
      "2024-09-08 20:02:13 Populating from Authority_Code_ADDRESS_TYPE_AUT_psv.psv\n",
      "2024-09-08 20:02:13 Populating from Authority_Code_LEVEL_TYPE_AUT_psv.psv\n",
      "2024-09-08 20:02:13 Populating from Authority_Code_LOCALITY_ALIAS_TYPE_AUT_psv.psv\n",
      "2024-09-08 20:02:13 Populating from Authority_Code_STREET_TYPE_AUT_psv.psv\n",
      "2024-09-08 20:02:13 Populating from Authority_Code_STREET_LOCALITY_ALIAS_TYPE_AUT_psv.psv\n",
      "2024-09-08 20:02:13 Populating from Authority_Code_ADDRESS_TYPE_AUT_psv.psv\n",
      "2024-09-08 20:02:13 Populating from Authority_Code_LOCALITY_ALIAS_TYPE_AUT_psv.psv\n",
      "2024-09-08 20:02:13 Populating from Authority_Code_STREET_CLASS_AUT_psv.psv\n",
      "2024-09-08 20:02:13 Populating from Authority_Code_STREET_LOCALITY_ALIAS_TYPE_AUT_psv.psv\n",
      "2024-09-08 20:02:13 Populating from Authority_Code_STREET_SUFFIX_AUT_psv.psv\n",
      "2024-09-08 20:02:13 Populating from Authority_Code_GEOCODED_LEVEL_TYPE_AUT_psv.psv\n",
      "2024-09-08 20:02:13 Populating from Authority_Code_LEVEL_TYPE_AUT_psv.psv\n",
      "2024-09-08 20:02:13 Populating from Authority_Code_STREET_TYPE_AUT_psv.psv\n",
      "2024-09-08 20:02:13 Populating from Authority_Code_FLAT_TYPE_AUT_psv.psv\n",
      "2024-09-08 20:02:13 Populating from Authority_Code_ADDRESS_ALIAS_TYPE_AUT_psv.psv\n",
      "2024-09-08 20:02:13 Populating from Authority_Code_ADDRESS_CHANGE_TYPE_AUT_psv.psv\n",
      "2024-09-08 20:02:13 Populating from Authority_Code_PS_JOIN_TYPE_AUT_psv.psv\n",
      "2024-09-08 20:02:13 Populating from Authority_Code_GEOCODE_TYPE_AUT_psv.psv\n",
      "2024-09-08 20:02:13 Populating from Authority_Code_GEOCODE_RELIABILITY_AUT_psv.psv\n",
      "2024-09-08 20:02:13 Populating from Authority_Code_LOCALITY_CLASS_AUT_psv.psv\n",
      "2024-09-08 20:02:13 Populating from Authority_Code_MB_MATCH_CODE_AUT_psv.psv\n",
      "2024-09-08 20:02:14 Populating from TAS_MB_2021_psv.psv\n",
      "2024-09-08 20:02:14 Populating from ACT_MB_2016_psv.psv\n",
      "2024-09-08 20:02:14 Populating from SA_MB_2021_psv.psv\n",
      "2024-09-08 20:02:15 Populating from WA_MB_2021_psv.psv\n",
      "2024-09-08 20:02:15 Populating from NT_STATE_psv.psv\n",
      "2024-09-08 20:02:15 Populating from NSW_MB_2016_psv.psv\n",
      "2024-09-08 20:02:18 Populating from SA_MB_2016_psv.psv\n",
      "2024-09-08 20:02:21 Populating from VIC_STATE_psv.psv\n",
      "2024-09-08 20:02:21 Populating from VIC_MB_2016_psv.psv\n",
      "2024-09-08 20:02:22 Populating from OT_MB_2016_psv.psv\n",
      "2024-09-08 20:02:22 Populating from NT_MB_2016_psv.psv\n",
      "2024-09-08 20:02:22 Populating from VIC_MB_2021_psv.psv\n",
      "2024-09-08 20:02:32 Populating from NSW_STATE_psv.psv\n",
      "2024-09-08 20:02:32 Populating from ACT_STATE_psv.psv\n",
      "2024-09-08 20:02:32 Populating from QLD_MB_2016_psv.psv\n",
      "2024-09-08 20:02:33 Populating from NT_MB_2021_psv.psv\n",
      "2024-09-08 20:02:34 Populating from ACT_MB_2021_psv.psv\n",
      "2024-09-08 20:02:35 Populating from TAS_MB_2016_psv.psv\n",
      "2024-09-08 20:02:36 Populating from SA_STATE_psv.psv\n",
      "2024-09-08 20:02:36 Populating from OT_STATE_psv.psv\n",
      "2024-09-08 20:02:36 Populating from WA_STATE_psv.psv\n",
      "2024-09-08 20:02:36 Populating from QLD_MB_2021_psv.psv\n",
      "2024-09-08 20:02:37 Populating from TAS_STATE_psv.psv\n",
      "2024-09-08 20:02:37 Populating from NSW_MB_2021_psv.psv\n",
      "2024-09-08 20:02:42 Populating from QLD_STATE_psv.psv\n",
      "2024-09-08 20:02:42 Populating from OT_MB_2021_psv.psv\n",
      "2024-09-08 20:02:42 Populating from WA_MB_2016_psv.psv\n",
      "2024-09-08 20:02:46 Populating from NT_ADDRESS_SITE_psv.psv\n",
      "2024-09-08 20:02:48 Populating from NT_LOCALITY_psv.psv\n",
      "2024-09-08 20:02:48 Populating from VIC_LOCALITY_psv.psv\n",
      "2024-09-08 20:02:49 Populating from VIC_ADDRESS_SITE_psv.psv\n",
      "2024-09-08 20:02:53 Populating from NSW_LOCALITY_psv.psv\n",
      "2024-09-08 20:02:54 Populating from NSW_ADDRESS_SITE_psv.psv\n",
      "2024-09-08 20:03:04 Populating from ACT_LOCALITY_psv.psv\n",
      "2024-09-08 20:03:04 Populating from ACT_ADDRESS_SITE_psv.psv\n",
      "2024-09-08 20:03:46 Populating from SA_ADDRESS_SITE_psv.psv\n",
      "2024-09-08 20:06:57 Populating from SA_LOCALITY_psv.psv\n",
      "2024-09-08 20:06:57 Populating from OT_LOCALITY_psv.psv\n",
      "2024-09-08 20:06:57 Populating from OT_ADDRESS_SITE_psv.psv\n",
      "2024-09-08 20:06:58 Populating from WA_LOCALITY_psv.psv\n",
      "2024-09-08 20:06:59 Populating from WA_ADDRESS_SITE_psv.psv\n",
      "2024-09-08 20:11:07 Populating from TAS_ADDRESS_SITE_psv.psv\n",
      "2024-09-08 20:12:03 Populating from TAS_LOCALITY_psv.psv\n",
      "2024-09-08 20:12:03 Populating from QLD_LOCALITY_psv.psv\n",
      "2024-09-08 20:12:03 Populating from QLD_ADDRESS_SITE_psv.psv\n",
      "2024-09-08 20:13:38 Populating from NT_LOCALITY_POINT_psv.psv\n",
      "2024-09-08 20:13:38 Populating from NT_STREET_LOCALITY_psv.psv\n",
      "2024-09-08 20:13:40 Populating from NT_LOCALITY_NEIGHBOUR_psv.psv\n",
      "2024-09-08 20:13:40 Populating from NT_LOCALITY_ALIAS_psv.psv\n",
      "2024-09-08 20:13:40 Populating from VIC_LOCALITY_NEIGHBOUR_psv.psv\n",
      "2024-09-08 20:13:43 Populating from VIC_LOCALITY_POINT_psv.psv\n",
      "2024-09-08 20:13:43 Populating from VIC_LOCALITY_ALIAS_psv.psv\n",
      "2024-09-08 20:13:44 Populating from VIC_STREET_LOCALITY_psv.psv\n",
      "2024-09-08 20:14:20 Populating from NSW_LOCALITY_NEIGHBOUR_psv.psv\n",
      "2024-09-08 20:14:25 Populating from NSW_STREET_LOCALITY_psv.psv\n",
      "2024-09-08 20:14:58 Populating from NSW_LOCALITY_POINT_psv.psv\n",
      "2024-09-08 20:14:59 Populating from NSW_LOCALITY_ALIAS_psv.psv\n",
      "2024-09-08 20:15:00 Populating from NT_ADDRESS_SITE_GEOCODE_psv.psv\n",
      "2024-09-08 20:15:19 Populating from ACT_LOCALITY_NEIGHBOUR_psv.psv\n",
      "2024-09-08 20:15:19 Populating from ACT_LOCALITY_POINT_psv.psv\n",
      "2024-09-08 20:15:20 Populating from ACT_LOCALITY_ALIAS_psv.psv\n",
      "2024-09-08 20:15:20 Populating from ACT_STREET_LOCALITY_psv.psv\n",
      "2024-09-08 20:15:21 Populating from ACT_ADDRESS_SITE_GEOCODE_psv.psv\n",
      "2024-09-08 20:15:45 Populating from SA_ADDRESS_SITE_GEOCODE_psv.psv\n",
      "2024-09-08 20:16:43 Populating from SA_LOCALITY_ALIAS_psv.psv\n",
      "2024-09-08 20:16:44 Populating from SA_LOCALITY_POINT_psv.psv\n",
      "2024-09-08 20:16:44 Populating from SA_LOCALITY_NEIGHBOUR_psv.psv\n",
      "2024-09-08 20:16:46 Populating from SA_STREET_LOCALITY_psv.psv\n",
      "2024-09-08 20:16:58 Populating from OT_LOCALITY_NEIGHBOUR_psv.psv\n",
      "2024-09-08 20:16:58 Populating from OT_LOCALITY_POINT_psv.psv\n",
      "2024-09-08 20:16:58 Populating from OT_LOCALITY_ALIAS_psv.psv\n",
      "2024-09-08 20:16:58 Populating from OT_STREET_LOCALITY_psv.psv\n",
      "2024-09-08 20:16:58 Populating from OT_ADDRESS_SITE_GEOCODE_psv.psv\n",
      "2024-09-08 20:16:59 Populating from WA_LOCALITY_ALIAS_psv.psv\n",
      "2024-09-08 20:16:59 Populating from WA_LOCALITY_NEIGHBOUR_psv.psv\n",
      "2024-09-08 20:17:01 Populating from WA_LOCALITY_POINT_psv.psv\n",
      "2024-09-08 20:17:01 Populating from WA_STREET_LOCALITY_psv.psv\n",
      "2024-09-08 20:17:17 Populating from WA_ADDRESS_SITE_GEOCODE_psv.psv\n",
      "2024-09-08 20:19:31 Populating from TAS_ADDRESS_SITE_GEOCODE_psv.psv\n",
      "2024-09-08 20:20:32 Populating from TAS_LOCALITY_POINT_psv.psv\n",
      "2024-09-08 20:20:32 Populating from TAS_STREET_LOCALITY_psv.psv\n",
      "2024-09-08 20:20:36 Populating from TAS_LOCALITY_ALIAS_psv.psv\n",
      "2024-09-08 20:20:36 Populating from TAS_LOCALITY_NEIGHBOUR_psv.psv\n",
      "2024-09-08 20:20:36 Populating from QLD_LOCALITY_POINT_psv.psv\n",
      "2024-09-08 20:20:37 Populating from QLD_LOCALITY_ALIAS_psv.psv\n",
      "2024-09-08 20:20:38 Populating from QLD_STREET_LOCALITY_psv.psv\n",
      "2024-09-08 20:20:38 Populating from QLD_LOCALITY_NEIGHBOUR_psv.psv\n",
      "2024-09-08 20:20:41 Populating from VIC_ADDRESS_SITE_GEOCODE_psv.psv\n",
      "2024-09-08 20:21:10 Populating from NT_STREET_LOCALITY_POINT_psv.psv\n",
      "2024-09-08 20:21:11 Populating from NT_ADDRESS_DETAIL_psv.psv\n",
      "2024-09-08 20:21:37 Populating from NT_STREET_LOCALITY_ALIAS_psv.psv\n",
      "2024-09-08 20:21:37 Populating from VIC_STREET_LOCALITY_POINT_psv.psv\n",
      "2024-09-08 20:21:58 Populating from VIC_ADDRESS_DETAIL_psv.psv\n",
      "2024-09-08 20:22:09 Populating from VIC_STREET_LOCALITY_ALIAS_psv.psv\n",
      "2024-09-08 20:22:11 Populating from NSW_STREET_LOCALITY_ALIAS_psv.psv\n",
      "2024-09-08 20:22:12 Populating from NSW_STREET_LOCALITY_POINT_psv.psv\n",
      "2024-09-08 20:22:41 Populating from ACT_STREET_LOCALITY_POINT_psv.psv\n",
      "2024-09-08 20:22:42 Populating from ACT_ADDRESS_DETAIL_psv.psv\n",
      "2024-09-08 20:23:44 Populating from ACT_STREET_LOCALITY_ALIAS_psv.psv\n",
      "2024-09-08 20:23:44 Populating from NSW_ADDRESS_DETAIL_psv.psv\n",
      "2024-09-08 20:40:27 Populating from NSW_ADDRESS_SITE_GEOCODE_psv.psv\n",
      "2024-09-08 20:45:58 Populating from SA_STREET_LOCALITY_POINT_psv.psv\n",
      "2024-09-08 20:46:09 Populating from SA_STREET_LOCALITY_ALIAS_psv.psv\n",
      "2024-09-08 20:46:10 Populating from SA_ADDRESS_DETAIL_psv.psv\n",
      "2024-09-08 20:48:12 Populating from OT_STREET_LOCALITY_ALIAS_psv.psv\n",
      "2024-09-08 20:48:12 Populating from OT_ADDRESS_DETAIL_psv.psv\n",
      "2024-09-08 20:48:13 Populating from OT_STREET_LOCALITY_POINT_psv.psv\n",
      "2024-09-08 20:48:13 Populating from WA_STREET_LOCALITY_ALIAS_psv.psv\n",
      "2024-09-08 20:48:13 Populating from WA_ADDRESS_DETAIL_psv.psv\n",
      "2024-09-08 20:51:09 Populating from WA_STREET_LOCALITY_POINT_psv.psv\n",
      "2024-09-08 20:51:22 Populating from TAS_STREET_LOCALITY_POINT_psv.psv\n",
      "2024-09-08 20:51:26 Populating from TAS_ADDRESS_DETAIL_psv.psv\n",
      "2024-09-08 20:52:52 Populating from TAS_STREET_LOCALITY_ALIAS_psv.psv\n",
      "2024-09-08 20:52:52 Populating from QLD_ADDRESS_SITE_GEOCODE_psv.psv\n",
      "2024-09-08 20:54:45 Populating from QLD_STREET_LOCALITY_ALIAS_psv.psv\n",
      "2024-09-08 20:54:47 Populating from QLD_STREET_LOCALITY_POINT_psv.psv\n",
      "2024-09-08 20:55:13 Populating from QLD_ADDRESS_DETAIL_psv.psv\n",
      "2024-09-08 20:56:33 Populating from NT_ADDRESS_DEFAULT_GEOCODE_psv.psv\n",
      "2024-09-08 20:56:52 Populating from NT_ADDRESS_ALIAS_psv.psv\n",
      "2024-09-08 20:56:53 Populating from NT_ADDRESS_MESH_BLOCK_2016_psv.psv\n",
      "2024-09-08 20:57:12 Populating from NT_ADDRESS_FEATURE_psv.psv\n",
      "2024-09-08 20:57:12 Populating from NT_ADDRESS_MESH_BLOCK_2021_psv.psv\n",
      "2024-09-08 20:57:32 Populating from NT_PRIMARY_SECONDARY_psv.psv\n",
      "2024-09-08 20:57:38 Populating from ACT_ADDRESS_FEATURE_psv.psv\n",
      "2024-09-08 20:57:38 Populating from ACT_ADDRESS_MESH_BLOCK_2021_psv.psv\n",
      "2024-09-08 20:58:24 Populating from ACT_ADDRESS_MESH_BLOCK_2016_psv.psv\n",
      "2024-09-08 20:59:10 Populating from ACT_ADDRESS_DEFAULT_GEOCODE_psv.psv\n",
      "2024-09-08 20:59:54 Populating from ACT_ADDRESS_ALIAS_psv.psv\n",
      "2024-09-08 20:59:55 Populating from ACT_PRIMARY_SECONDARY_psv.psv\n",
      "2024-09-08 21:00:12 Populating from VIC_ADDRESS_MESH_BLOCK_2016_psv.psv\n",
      "2024-09-08 21:03:26 Populating from VIC_ADDRESS_ALIAS_psv.psv\n",
      "2024-09-08 21:04:12 Populating from VIC_ADDRESS_FEATURE_psv.psv\n",
      "2024-09-08 21:04:20 Populating from VIC_PRIMARY_SECONDARY_psv.psv\n",
      "2024-09-08 21:08:43 Populating from VIC_ADDRESS_DEFAULT_GEOCODE_psv.psv\n",
      "2024-09-08 21:09:57 Populating from VIC_ADDRESS_MESH_BLOCK_2021_psv.psv\n",
      "2024-09-08 21:14:30 Populating from NSW_ADDRESS_FEATURE_psv.psv\n",
      "2024-09-08 21:14:43 Populating from NSW_ADDRESS_MESH_BLOCK_2021_psv.psv\n",
      "2024-09-08 21:23:05 Populating from NSW_ADDRESS_DEFAULT_GEOCODE_psv.psv\n",
      "2024-09-08 21:24:22 Populating from NSW_ADDRESS_ALIAS_psv.psv\n",
      "2024-09-08 21:25:23 Populating from NSW_ADDRESS_MESH_BLOCK_2016_psv.psv\n",
      "2024-09-08 21:32:12 Populating from NSW_PRIMARY_SECONDARY_psv.psv\n",
      "2024-09-08 21:38:37 Populating from OT_ADDRESS_ALIAS_psv.psv\n",
      "2024-09-08 21:38:37 Populating from OT_ADDRESS_DEFAULT_GEOCODE_psv.psv\n",
      "2024-09-08 21:38:38 Populating from OT_ADDRESS_MESH_BLOCK_2016_psv.psv\n",
      "2024-09-08 21:38:39 Populating from OT_PRIMARY_SECONDARY_psv.psv\n",
      "2024-09-08 21:38:39 Populating from OT_ADDRESS_MESH_BLOCK_2021_psv.psv\n",
      "2024-09-08 21:38:40 Populating from OT_ADDRESS_FEATURE_psv.psv\n",
      "2024-09-08 21:38:40 Populating from SA_ADDRESS_FEATURE_psv.psv\n",
      "2024-09-08 21:38:49 Populating from SA_ADDRESS_ALIAS_psv.psv\n",
      "2024-09-08 21:38:59 Populating from SA_ADDRESS_MESH_BLOCK_2016_psv.psv\n",
      "2024-09-08 21:40:21 Populating from SA_PRIMARY_SECONDARY_psv.psv\n",
      "2024-09-08 21:41:01 Populating from SA_ADDRESS_DEFAULT_GEOCODE_psv.psv\n",
      "2024-09-08 21:42:31 Populating from SA_ADDRESS_MESH_BLOCK_2021_psv.psv\n",
      "2024-09-08 21:43:01 Populating from TAS_ADDRESS_FEATURE_psv.psv\n",
      "2024-09-08 21:43:01 Populating from TAS_ADDRESS_MESH_BLOCK_2016_psv.psv\n",
      "2024-09-08 21:44:04 Populating from TAS_PRIMARY_SECONDARY_psv.psv\n",
      "2024-09-08 21:44:15 Populating from TAS_ADDRESS_MESH_BLOCK_2021_psv.psv\n",
      "2024-09-08 21:44:43 Populating from TAS_ADDRESS_ALIAS_psv.psv\n",
      "2024-09-08 21:44:44 Populating from TAS_ADDRESS_DEFAULT_GEOCODE_psv.psv\n",
      "2024-09-08 21:45:19 Populating from WA_ADDRESS_DEFAULT_GEOCODE_psv.psv\n",
      "2024-09-08 21:45:45 Populating from WA_ADDRESS_ALIAS_psv.psv\n",
      "2024-09-08 21:45:57 Populating from WA_ADDRESS_FEATURE_psv.psv\n",
      "2024-09-08 21:45:59 Populating from WA_ADDRESS_MESH_BLOCK_2021_psv.psv\n",
      "2024-09-08 21:46:04 Populating from WA_ADDRESS_MESH_BLOCK_2016_psv.psv\n",
      "2024-09-08 21:50:06 Populating from WA_PRIMARY_SECONDARY_psv.psv\n",
      "2024-09-08 21:50:44 Populating from QLD_ADDRESS_MESH_BLOCK_2016_psv.psv\n",
      "2024-09-08 21:50:48 Populating from QLD_ADDRESS_FEATURE_psv.psv\n",
      "2024-09-08 21:51:00 Populating from QLD_ADDRESS_MESH_BLOCK_2021_psv.psv\n",
      "2024-09-08 21:51:09 Populating from QLD_PRIMARY_SECONDARY_psv.psv\n",
      "2024-09-08 21:54:08 Populating from QLD_ADDRESS_DEFAULT_GEOCODE_psv.psv\n",
      "2024-09-08 22:01:00 Populating from QLD_ADDRESS_ALIAS_psv.psv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "import psycopg2\n",
    "import concurrent.futures\n",
    "\n",
    "from collections import defaultdict, deque\n",
    "from datetime import datetime\n",
    "from lib import notebook_constants as nc\n",
    "from threading import Lock\n",
    "\n",
    "schema = 'gnaf'\n",
    "WORKER_COUNT = os.cpu_count()\n",
    "BATCH_SIZE = 64000 / WORKER_COUNT # idk what I'm doing here tbh.\n",
    "\n",
    "def get_table_name(file):\n",
    "    file = os.path.splitext(os.path.basename(file))[0]\n",
    "    sidx = 15 if file.startswith('Authority_Code') else file.find('_')+1\n",
    "    return file[sidx:file.rfind('_')]\n",
    "\n",
    "def get_batches(batch_size, reader):\n",
    "    batch = []\n",
    "    for row in reader:\n",
    "        row = [(None if v == \"\" else v) for v in (v.strip() for v in row)]\n",
    "        batch.append(row)\n",
    "        \n",
    "        if len(batch) >= batch_size:\n",
    "            yield batch\n",
    "            batch = [] \n",
    "    if batch:\n",
    "        yield batch\n",
    "\n",
    "def populate_file(file):\n",
    "    table_name = get_table_name(file)\n",
    "    with gnaf_db.connect() as conn:\n",
    "        cursor = conn.cursor()\n",
    "        with open(file, 'r') as f:\n",
    "            time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            print(f\"{time} Populating from {os.path.basename(file)}\")\n",
    "            reader = csv.reader(f, delimiter='|')\n",
    "            headers = next(reader)\n",
    "            insert_query = f\"\"\"\n",
    "                INSERT INTO {schema}.{table_name} ({', '.join(headers)}) \n",
    "                VALUES ({', '.join(['%s'] * len(headers))})\n",
    "                ON CONFLICT DO NOTHING\n",
    "            \"\"\"\n",
    "            \n",
    "            for batch_index, batch in enumerate(get_batches(BATCH_SIZE, reader)):\n",
    "                try:\n",
    "                    cursor.executemany(insert_query, batch)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error inserting batch {batch_index + 1} into {table_name}: {e}\")\n",
    "                    raise e\n",
    "            conn.commit()\n",
    "\n",
    "def get_ordered_files(dependencies, all_files):\n",
    "    file_sizes = { file: os.path.getsize(file) for file in all_files }\n",
    "    total_blocked_sizes = defaultdict(int)\n",
    "    \n",
    "    def dfs(file, visited):\n",
    "        if file in visited:\n",
    "            return 0\n",
    "        visited.add(file)\n",
    "        total_size = file_sizes[file]\n",
    "        for dep in dependencies[file]:\n",
    "            total_size += dfs(dep, visited)\n",
    "        return total_size\n",
    "    \n",
    "    for file in dependencies:\n",
    "        visited = set()\n",
    "        total_blocked_sizes[file] = dfs(file, visited)\n",
    "        \n",
    "    # Sort files based on the total blocked sizes, with higher sizes first\n",
    "    return sorted(all_files, key=lambda f: total_blocked_sizes[f], reverse=True)\n",
    "\n",
    "def worker(file_queue, all_files, dependency_count, lock, dependency_completed):\n",
    "    while True:\n",
    "        with lock:\n",
    "            if not file_queue:\n",
    "                break\n",
    "            file = file_queue.popleft()\n",
    "        \n",
    "        populate_file(file)\n",
    "        \n",
    "        with lock:\n",
    "            dependency_completed.add(file)\n",
    "            \n",
    "            for d in dependency_count:\n",
    "                if file in dependency_count[d]:\n",
    "                    dependency_count[d].remove(file)\n",
    "                    \n",
    "            ready_files = [f for f in all_files if not dependency_count[f]]\n",
    "            \n",
    "            for ready_file in ready_files:\n",
    "                if ready_file not in file_queue:\n",
    "                    file_queue.append(ready_file)\n",
    "                    all_files.remove(ready_file)\n",
    "\n",
    "authority_files = glob.glob(f'{gnaf_pub.psv_dir}/Authority Code/*.psv')\n",
    "\n",
    "standard_prefix = f'{gnaf_pub.psv_dir}/Standard'\n",
    "standard_files = [\n",
    "    f'{standard_prefix}/{s}_{t}_psv.psv' \n",
    "    for t in [\n",
    "        'STATE', 'ADDRESS_SITE', 'MB_2016', 'MB_2021', 'LOCALITY',\n",
    "        'LOCALITY_ALIAS', 'LOCALITY_NEIGHBOUR', 'LOCALITY_POINT',\n",
    "        'STREET_LOCALITY', 'STREET_LOCALITY_ALIAS', 'STREET_LOCALITY_POINT',\n",
    "        'ADDRESS_DETAIL', 'ADDRESS_SITE_GEOCODE', 'ADDRESS_ALIAS', \n",
    "        'ADDRESS_DEFAULT_GEOCODE', 'ADDRESS_FEATURE', \n",
    "        'ADDRESS_MESH_BLOCK_2016', 'ADDRESS_MESH_BLOCK_2021',\n",
    "        'PRIMARY_SECONDARY',\n",
    "    ] \n",
    "    for s in ['NSW', 'VIC', 'QLD', 'WA', 'SA', 'TAS', 'NT', 'OT', 'ACT'] \n",
    "]\n",
    "\n",
    "standard_deps = { f: set() for f in authority_files } | { \n",
    "    f'{p}/{s}_{t}_psv.psv': { f'{p}/{s}_{d}_psv.psv' for d in ds } | set(authority_files)\n",
    "    \n",
    "    for t, ds in ({\n",
    "        'STATE': [],\n",
    "        'ADDRESS_SITE': ['STATE'],\n",
    "        'MB_2016': [],\n",
    "        'MB_2021': [],\n",
    "        'LOCALITY': ['STATE'],\n",
    "        'LOCALITY_ALIAS': ['LOCALITY'],\n",
    "        'LOCALITY_NEIGHBOUR': ['LOCALITY'],\n",
    "        'LOCALITY_POINT': ['LOCALITY'],\n",
    "        'STREET_LOCALITY': ['LOCALITY'],\n",
    "        'STREET_LOCALITY_ALIAS': ['STREET_LOCALITY'],\n",
    "        'STREET_LOCALITY_POINT': ['STREET_LOCALITY'],\n",
    "        'ADDRESS_DETAIL': ['ADDRESS_SITE', 'STATE', 'LOCALITY', 'STREET_LOCALITY'],\n",
    "        'ADDRESS_SITE_GEOCODE': ['ADDRESS_SITE'],\n",
    "        'ADDRESS_ALIAS': ['ADDRESS_DETAIL'],\n",
    "        'ADDRESS_DEFAULT_GEOCODE': ['ADDRESS_DETAIL'],\n",
    "        'ADDRESS_FEATURE': ['ADDRESS_DETAIL'],\n",
    "        'ADDRESS_MESH_BLOCK_2016': ['ADDRESS_DETAIL', 'MB_2016'],\n",
    "        'ADDRESS_MESH_BLOCK_2021': ['ADDRESS_DETAIL', 'MB_2021'],\n",
    "        'PRIMARY_SECONDARY': ['ADDRESS_DETAIL'],\n",
    "    }).items()\n",
    "    for s in ['NSW', 'VIC', 'QLD', 'WA', 'SA', 'TAS', 'NT', 'OT', 'ACT'] \n",
    "    for p in [standard_prefix]\n",
    "}\n",
    "\n",
    "lock = Lock()\n",
    "all_files = { *authority_files, *standard_files }\n",
    "dependency_count = {k: set(v) for k, v in standard_deps.items()}\n",
    "dependency_completed = set()\n",
    "file_queue = deque()\n",
    "file_queue.extend(f for f in get_ordered_files(dependency_count, all_files) if not dependency_count[f])\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=WORKER_COUNT) as executor:\n",
    "    futures = [executor.submit(worker, file_queue, all_files, dependency_count, lock, dependency_completed) for _ in range(WORKER_COUNT)]\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        future.result()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68a8268-cf59-4249-ac33-653bd5c2afc1",
   "metadata": {},
   "source": [
    "## Done\n",
    "\n",
    "We've now built up the dataset, lets analysis what we got and show the contents of the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23e887af-b30e-40da-ae8b-ae0c6d779816",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table nsw_valuer_general.source_file has 128 rows\n",
      "Table nsw_valuer_general.source has 2702450 rows\n",
      "Table nsw_valuer_general.district has 128 rows\n",
      "Table nsw_valuer_general.suburb has 5075 rows\n",
      "Table nsw_valuer_general.street has 128422 rows\n",
      "Table nsw_valuer_general.property has 2702450 rows\n",
      "Table nsw_valuer_general.property_description has 2702450 rows\n",
      "Table nsw_valuer_general.valuations has 13512250 rows\n",
      "Table nsw_valuer_general.land_parcel_link has 4245674 rows\n",
      "Table gnaf.address_alias has 861514 rows\n",
      "Table gnaf.address_detail has 16491363 rows\n",
      "Table gnaf.geocode_type_aut has 29 rows\n",
      "Table gnaf.state has 9 rows\n",
      "Table gnaf.street_locality has 754249 rows\n",
      "Table gnaf.address_alias_type_aut has 8 rows\n",
      "Table gnaf.address_change_type_aut has 511 rows\n",
      "Table gnaf.address_default_geocode has 16491363 rows\n",
      "Table gnaf.address_feature has 211931 rows\n",
      "Table gnaf.address_mesh_block_2016 has 16491363 rows\n",
      "Table gnaf.address_mesh_block_2021 has 16491363 rows\n",
      "Table gnaf.address_site has 16491363 rows\n",
      "Table gnaf.address_site_geocode has 20196170 rows\n",
      "Table gnaf.address_type_aut has 3 rows\n",
      "Table gnaf.flat_type_aut has 54 rows\n",
      "Table gnaf.geocoded_level_type_aut has 8 rows\n",
      "Table gnaf.geocode_reliability_aut has 6 rows\n",
      "Table gnaf.level_type_aut has 16 rows\n",
      "Table gnaf.locality has 17566 rows\n",
      "Table gnaf.locality_alias has 29546 rows\n",
      "Table gnaf.locality_alias_type_aut has 2 rows\n",
      "Table gnaf.locality_class_aut has 9 rows\n",
      "Table gnaf.locality_neighbour has 89366 rows\n",
      "Table gnaf.locality_point has 17566 rows\n",
      "Table gnaf.mb_2016 has 358122 rows\n",
      "Table gnaf.mb_2021 has 368285 rows\n",
      "Table gnaf.mb_match_code_aut has 5 rows\n",
      "Table gnaf.primary_secondary has 4817892 rows\n",
      "Table gnaf.ps_join_type_aut has 2 rows\n",
      "Table gnaf.street_class_aut has 2 rows\n",
      "Table gnaf.street_locality_alias has 40050 rows\n",
      "Table gnaf.street_locality_alias_type_aut has 2 rows\n",
      "Table gnaf.street_locality_point has 713093 rows\n",
      "Table gnaf.street_suffix_aut has 19 rows\n",
      "Table gnaf.street_type_aut has 276 rows\n",
      "Table abs_main_structures.state has 10 rows\n",
      "Table abs_main_structures.gccsa has 35 rows\n",
      "Table abs_main_structures.sa4 has 108 rows\n",
      "Table abs_main_structures.sa3 has 359 rows\n",
      "Table abs_main_structures.sa2 has 2473 rows\n",
      "Table abs_main_structures.sa1 has 61845 rows\n",
      "Table abs_main_structures.meshblock has 368286 rows\n",
      "Table abs_main_structures.dzn has 0 rows\n",
      "Table non_abs_main_structures.localities has 15353 rows\n",
      "Table non_abs_main_structures.state_electoral_division_2021 has 452 rows\n",
      "Table non_abs_main_structures.state_electoral_division_2022 has 452 rows\n",
      "Table non_abs_main_structures.state_electoral_division_2024 has 452 rows\n",
      "Table non_abs_main_structures.federal_electoral_division_2021 has 170 rows\n",
      "Table non_abs_main_structures.lga_2021 has 566 rows\n",
      "Table non_abs_main_structures.lga_2022 has 566 rows\n",
      "Table non_abs_main_structures.dzn has 9329 rows\n",
      "Table non_abs_main_structures.lga_2023 has 566 rows\n",
      "Table non_abs_main_structures.lga_2024 has 566 rows\n",
      "Table non_abs_main_structures.post_code has 2644 rows\n"
     ]
    }
   ],
   "source": [
    "with gnaf_db.connect() as conn:\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    for schema in ['nsw_valuer_general', 'gnaf', 'abs_main_structures', 'non_abs_main_structures']:\n",
    "        # Get the list of all tables\n",
    "        cursor.execute(f\"\"\"\n",
    "            SELECT table_name\n",
    "            FROM information_schema.tables\n",
    "            WHERE table_schema = '{schema}'\n",
    "        \"\"\")\n",
    "        tables = cursor.fetchall()\n",
    "    \n",
    "        # Get row count for each table\n",
    "        for table in tables:\n",
    "            cursor.execute(f'SELECT COUNT(*) FROM {schema}.{table[0]}')\n",
    "            count = cursor.fetchone()[0]\n",
    "            print(f\"Table {schema}.{table[0]} has {count} rows\")\n",
    "    \n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982acfc4-7182-4a73-aa4b-fc9aae3db7d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
