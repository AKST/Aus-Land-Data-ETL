{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e64a6769-685f-4029-8daa-4780d096ef81",
   "metadata": {},
   "source": [
    "# Dataset ingestion\n",
    "\n",
    "This jupyter noteebook ingests the [Geocoded National Address File][gnaf] ([GNAF][gnaf]) from [data.gov.au](data.gov.au). It also downloads the [land values for NSW][nswlv], and ABS shapefiles \n",
    "\n",
    "It loads it all this data into a PostgreSQL database in a docker container, treating it like a disposable sqlite data store. It also downloads the ABS shape files as well as the \n",
    "\n",
    "Here we are going to ingest all the data necessary in order to assess land by land values, and filter them by address information. \n",
    "\n",
    "### The Steps\n",
    "\n",
    "1. Download static assets and datasets\n",
    "2. Setup a docker container with postgresql with GIS capabilities.\n",
    "3. Ingest the [ABS shape files][abssf]\n",
    "4. Ingest the latest [NSW valuer general land values][nswlv].\n",
    "5. Ingest the [Geocoded National Address File][gnaf] ([GNAF][gnaf]) dataset\n",
    "6. Link NSW Valuer General data with GNAF dataset\n",
    "\n",
    "[gnaf]: https://data.gov.au/data/dataset/geocoded-national-address-file-g-naf\n",
    "[nswlv]: https://www.valuergeneral.nsw.gov.au/land_value_summaries/lv.php\n",
    "[abssf]: https://www.abs.gov.au/statistics/standards/australian-statistical-geography-standard-asgs-edition-3/jul2021-jun2026/access-and-downloads/digital-boundary-files\n",
    "\n",
    "### Note\n",
    "\n",
    "- Make sure docker is running first.\n",
    "\n",
    "### Warning\n",
    "\n",
    "Do not connect this to another database unless you've taken the time to update this, as it'll drop the existing database. I suggest instead take what you need from this script and disregard the rest. DO NOT USE DATABASE CREDENTIALS HERE FOR ANY OTHER STORE (especailly anything with drop permissions).\n",
    "\n",
    "It also executes sql from a zip file downloaded from an external source.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a77f1d8-22ff-4aa2-8740-58dd6b05c461",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "These are some fields to configure if you wish to configure how the data is injected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab987146-dde7-4ddc-bd9c-916596d4badb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import notebook_constants as nc\n",
    "\n",
    "GLOBAL_FLAGS = {\n",
    "    # If you mark this as true, the table `nsw_valuer_general.raw_entries`\n",
    "    # will be dropped. If you have space limitations and no desire to debug\n",
    "    # the data than dropping this makes sense. If you wish to debug some values\n",
    "    # then keeping this around may make some sense.\n",
    "    'drop_raw_nsw_valuer_general_entries': True,\n",
    "    'reinitialise_container': True,\n",
    "}\n",
    "\n",
    "db_conf = nc.gnaf_dbconf\n",
    "db_name = nc.gnaf_dbname\n",
    "\n",
    "docker_container_name = 'gnaf_db_prod'\n",
    "docker_image_tag = \"20240908_19_53\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48db8607-3ea8-43c1-9b87-3af22555c2b4",
   "metadata": {},
   "source": [
    "## Download Static Files\n",
    "\n",
    "Here we are downloading static files, as well as fetching the most recently published land values from the valuer generals website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "320cd7fe-f03d-4230-9e0f-a2ef302a91b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-01 19:24:47,706 - INFO - Checking Target \"abs_main_structures.zip\"\n",
      "2024-10-01 19:24:47,707 - INFO - Checking Target \"non_abs_shape.zip\"\n",
      "2024-10-01 19:24:47,708 - INFO - Checking Target \"g-naf_aug24_allstates_gda2020_psv_1016.zip\"\n",
      "2024-10-01 19:24:47,708 - INFO - Checking Target \"nswvg_lv_01_Oct_2024.zip\"\n",
      "2024-10-01 19:24:47,708 - INFO - Checking Target \"nswvg_wps_01_Jan_2024.zip\"\n",
      "2024-10-01 19:24:47,709 - INFO - Checking Target \"nswvg_wps_08_Jan_2024.zip\"\n",
      "2024-10-01 19:24:47,710 - INFO - Checking Target \"nswvg_wps_15_Jan_2024.zip\"\n",
      "2024-10-01 19:24:47,710 - INFO - Checking Target \"nswvg_wps_22_Jan_2024.zip\"\n",
      "2024-10-01 19:24:47,711 - INFO - Checking Target \"nswvg_wps_29_Jan_2024.zip\"\n",
      "2024-10-01 19:24:47,711 - INFO - Checking Target \"nswvg_wps_05_Feb_2024.zip\"\n",
      "2024-10-01 19:24:47,712 - INFO - Checking Target \"nswvg_wps_12_Feb_2024.zip\"\n",
      "2024-10-01 19:24:47,712 - INFO - Checking Target \"nswvg_wps_19_Feb_2024.zip\"\n",
      "2024-10-01 19:24:47,713 - INFO - Checking Target \"nswvg_wps_26_Feb_2024.zip\"\n",
      "2024-10-01 19:24:47,713 - INFO - Checking Target \"nswvg_wps_04_Mar_2024.zip\"\n",
      "2024-10-01 19:24:47,714 - INFO - Checking Target \"nswvg_wps_11_Mar_2024.zip\"\n",
      "2024-10-01 19:24:47,714 - INFO - Checking Target \"nswvg_wps_18_Mar_2024.zip\"\n",
      "2024-10-01 19:24:47,715 - INFO - Checking Target \"nswvg_wps_25_Mar_2024.zip\"\n",
      "2024-10-01 19:24:47,715 - INFO - Checking Target \"nswvg_wps_01_Apr_2024.zip\"\n",
      "2024-10-01 19:24:47,717 - INFO - Checking Target \"nswvg_wps_08_Apr_2024.zip\"\n",
      "2024-10-01 19:24:47,718 - INFO - Checking Target \"nswvg_wps_15_Apr_2024.zip\"\n",
      "2024-10-01 19:24:47,718 - INFO - Checking Target \"nswvg_wps_22_Apr_2024.zip\"\n",
      "2024-10-01 19:24:47,719 - INFO - Checking Target \"nswvg_wps_29_Apr_2024.zip\"\n",
      "2024-10-01 19:24:47,720 - INFO - Checking Target \"nswvg_wps_06_May_2024.zip\"\n",
      "2024-10-01 19:24:47,720 - INFO - Checking Target \"nswvg_wps_13_May_2024.zip\"\n",
      "2024-10-01 19:24:47,721 - INFO - Checking Target \"nswvg_wps_20_May_2024.zip\"\n",
      "2024-10-01 19:24:47,721 - INFO - Checking Target \"nswvg_wps_27_May_2024.zip\"\n",
      "2024-10-01 19:24:47,722 - INFO - Checking Target \"nswvg_wps_03_Jun_2024.zip\"\n",
      "2024-10-01 19:24:47,723 - INFO - Checking Target \"nswvg_wps_10_Jun_2024.zip\"\n",
      "2024-10-01 19:24:47,723 - INFO - Checking Target \"nswvg_wps_17_Jun_2024.zip\"\n",
      "2024-10-01 19:24:47,724 - INFO - Checking Target \"nswvg_wps_24_Jun_2024.zip\"\n",
      "2024-10-01 19:24:47,724 - INFO - Checking Target \"nswvg_wps_01_Jul_2024.zip\"\n",
      "2024-10-01 19:24:47,725 - INFO - Checking Target \"nswvg_wps_08_Jul_2024.zip\"\n",
      "2024-10-01 19:24:47,725 - INFO - Checking Target \"nswvg_wps_15_Jul_2024.zip\"\n",
      "2024-10-01 19:24:47,726 - INFO - Checking Target \"nswvg_wps_22_Jul_2024.zip\"\n",
      "2024-10-01 19:24:47,726 - INFO - Checking Target \"nswvg_wps_29_Jul_2024.zip\"\n",
      "2024-10-01 19:24:47,727 - INFO - Checking Target \"nswvg_wps_05_Aug_2024.zip\"\n",
      "2024-10-01 19:24:47,727 - INFO - Checking Target \"nswvg_wps_12_Aug_2024.zip\"\n",
      "2024-10-01 19:24:47,728 - INFO - Checking Target \"nswvg_wps_19_Aug_2024.zip\"\n",
      "2024-10-01 19:24:47,728 - INFO - Checking Target \"nswvg_wps_26_Aug_2024.zip\"\n",
      "2024-10-01 19:24:47,729 - INFO - Checking Target \"nswvg_wps_02_Sep_2024.zip\"\n",
      "2024-10-01 19:24:47,729 - INFO - Checking Target \"nswvg_wps_09_Sep_2024.zip\"\n",
      "2024-10-01 19:24:47,730 - INFO - Checking Target \"nswvg_wps_16_Sep_2024.zip\"\n",
      "2024-10-01 19:24:47,730 - INFO - Checking Target \"nswvg_wps_23_Sep_2024.zip\"\n",
      "2024-10-01 19:24:47,731 - INFO - Checking Target \"nswvg_wps_30_Sep_2024.zip\"\n",
      "2024-10-01 19:24:47,731 - INFO - Checking Target \"nswvg_aps_1990.zip\"\n",
      "2024-10-01 19:24:47,732 - INFO - Checking Target \"nswvg_aps_1991.zip\"\n",
      "2024-10-01 19:24:47,732 - INFO - Checking Target \"nswvg_aps_1992.zip\"\n",
      "2024-10-01 19:24:47,733 - INFO - Checking Target \"nswvg_aps_1993.zip\"\n",
      "2024-10-01 19:24:47,734 - INFO - Checking Target \"nswvg_aps_1994.zip\"\n",
      "2024-10-01 19:24:47,735 - INFO - Checking Target \"nswvg_aps_1995.zip\"\n",
      "2024-10-01 19:24:47,735 - INFO - Checking Target \"nswvg_aps_1996.zip\"\n",
      "2024-10-01 19:24:47,736 - INFO - Checking Target \"nswvg_aps_1997.zip\"\n",
      "2024-10-01 19:24:47,736 - INFO - Checking Target \"nswvg_aps_1998.zip\"\n",
      "2024-10-01 19:24:47,737 - INFO - Checking Target \"nswvg_aps_1999.zip\"\n",
      "2024-10-01 19:24:47,737 - INFO - Checking Target \"nswvg_aps_2000.zip\"\n",
      "2024-10-01 19:24:47,738 - INFO - Checking Target \"nswvg_aps_2001.zip\"\n",
      "2024-10-01 19:24:47,738 - INFO - Checking Target \"nswvg_aps_2002.zip\"\n",
      "2024-10-01 19:24:47,739 - INFO - Checking Target \"nswvg_aps_2003.zip\"\n",
      "2024-10-01 19:24:47,739 - INFO - Checking Target \"nswvg_aps_2004.zip\"\n",
      "2024-10-01 19:24:47,740 - INFO - Checking Target \"nswvg_aps_2005.zip\"\n",
      "2024-10-01 19:24:47,740 - INFO - Checking Target \"nswvg_aps_2006.zip\"\n",
      "2024-10-01 19:24:47,740 - INFO - Checking Target \"nswvg_aps_2007.zip\"\n",
      "2024-10-01 19:24:47,741 - INFO - Checking Target \"nswvg_aps_2008.zip\"\n",
      "2024-10-01 19:24:47,742 - INFO - Checking Target \"nswvg_aps_2009.zip\"\n",
      "2024-10-01 19:24:47,742 - INFO - Checking Target \"nswvg_aps_2010.zip\"\n",
      "2024-10-01 19:24:47,743 - INFO - Checking Target \"nswvg_aps_2011.zip\"\n",
      "2024-10-01 19:24:47,743 - INFO - Checking Target \"nswvg_aps_2012.zip\"\n",
      "2024-10-01 19:24:47,744 - INFO - Checking Target \"nswvg_aps_2013.zip\"\n",
      "2024-10-01 19:24:47,744 - INFO - Checking Target \"nswvg_aps_2014.zip\"\n",
      "2024-10-01 19:24:47,744 - INFO - Checking Target \"nswvg_aps_2015.zip\"\n",
      "2024-10-01 19:24:47,745 - INFO - Checking Target \"nswvg_aps_2016.zip\"\n",
      "2024-10-01 19:24:47,745 - INFO - Checking Target \"nswvg_aps_2017.zip\"\n",
      "2024-10-01 19:24:47,746 - INFO - Checking Target \"nswvg_aps_2018.zip\"\n",
      "2024-10-01 19:24:47,746 - INFO - Checking Target \"nswvg_aps_2019.zip\"\n",
      "2024-10-01 19:24:47,747 - INFO - Checking Target \"nswvg_aps_2020.zip\"\n",
      "2024-10-01 19:24:47,748 - INFO - Checking Target \"nswvg_aps_2021.zip\"\n",
      "2024-10-01 19:24:47,748 - INFO - Checking Target \"nswvg_aps_2022.zip\"\n",
      "2024-10-01 19:24:47,749 - INFO - Checking Target \"nswvg_aps_2023.zip\"\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from lib.service.io import IoService\n",
    "from lib.tasks.fetch_static_files import initialise, get_session\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "io = IoService.create(None)\n",
    "async with get_session(io) as session:\n",
    "    environment = await initialise(io, session)\n",
    "\n",
    "land_value_dis = environment.land_value\n",
    "w_sale_price = environment.sale_price_weekly\n",
    "a_sale_price = environment.sale_price_annual\n",
    "gnaf_dis = environment.gnaf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bec1a4-8640-40ca-8c07-aafd1dd3a8e8",
   "metadata": {},
   "source": [
    "## Create Container with Database\n",
    "\n",
    "Here we are creating a container in docker from an image that uses the postgres image, which also installs a few extensions.\n",
    "\n",
    "### Note\n",
    "\n",
    "This notebook this is designed to be run more than once, so it'll throw away any existing container and database before creating a new one. After getting rid of any container using the same identifer, it'll create a new one and pull the relevant image if it's not already installed. It'll wait till the postgres instance is live then create the database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f709233-e243-468b-971e-df23bfcc0113",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/code/jupyter/notebooks/20240907, vg/lib/gnaf_db.py:135\u001b[0m, in \u001b[0;36mGnafDb.wait_till_running\u001b[0;34m(self, interval, timeout)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[43mpsycopg2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdbname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpostgres\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     conn\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/code/jupyter/env/lib/python3.12/site-packages/psycopg2/__init__.py:122\u001b[0m, in \u001b[0;36mconnect\u001b[0;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m dsn \u001b[38;5;241m=\u001b[39m _ext\u001b[38;5;241m.\u001b[39mmake_dsn(dsn, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 122\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconnection_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwasync\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cursor_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOperationalError\u001b[0m: connection to server at \"localhost\" (::1), port 5432 failed: server closed the connection unexpectedly\n\tThis probably means the server terminated abnormally\n\tbefore or while processing the request.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskipping container initialisation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m gnaf_db \u001b[38;5;241m=\u001b[39m GnafDb\u001b[38;5;241m.\u001b[39mcreate(db_conf, db_name)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mgnaf_db\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_till_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m GLOBAL_FLAGS[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreinitialise_container\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     19\u001b[0m     gnaf_db\u001b[38;5;241m.\u001b[39minit_schema(gnaf_dis\u001b[38;5;241m.\u001b[39mpublication)\n",
      "File \u001b[0;32m~/code/jupyter/notebooks/20240907, vg/lib/gnaf_db.py:141\u001b[0m, in \u001b[0;36mGnafDb.wait_till_running\u001b[0;34m(self, interval, timeout)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time \u001b[38;5;241m>\u001b[39m timeout:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m--> 141\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterval\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from lib.gnaf_db import GnafDb, GnafContainer, GnafImage\n",
    "from lib import notebook_constants as nc\n",
    "\n",
    "if GLOBAL_FLAGS['reinitialise_container']:\n",
    "    image = GnafImage.create(tag=docker_image_tag)\n",
    "    image.prepare()\n",
    "    \n",
    "    container = GnafContainer.create(container_name=docker_container_name, image=image)\n",
    "    container.clean()\n",
    "    container.prepare(db_conf, db_name)\n",
    "    container.start()\n",
    "else:\n",
    "    print('skipping container initialisation')\n",
    "\n",
    "gnaf_db = GnafDb.create(db_conf, db_name)\n",
    "gnaf_db.wait_till_running()\n",
    "\n",
    "if GLOBAL_FLAGS['reinitialise_container']:\n",
    "    gnaf_db.init_schema(gnaf_dis.publication)\n",
    "else:\n",
    "    print('skipping DB initialisation')\n",
    "    raise Exception()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdd42a1-6d1e-4fe2-a3d3-20e7526dc021",
   "metadata": {},
   "source": [
    "## Consume the ABS Shapefiles\n",
    "\n",
    "The [ABS provides a number of shape files][all abs shape files], we're going focus on 2 main sets of shapes. The **ABS Main Structures** which is stuff like SA1, 2, 3 & 4 along with greater cities, meshblocks, and states. As well as **Non ABS Main Structures** which is stuff like electoral divisions, suburbs post codes etc.\n",
    "\n",
    "[all abs shape files]: https://www.abs.gov.au/statistics/standards/australian-statistical-geography-standard-asgs-edition-3/jul2021-jun2026/access-and-downloads/digital-boundary-files\n",
    "\n",
    "### ABS Main Structures \n",
    "\n",
    "Any address or region we look up in the GNAF dataset, we want to visualise. The ABS has a few different geographic groups which we can visualise the data against, but each address in the GNAF dataset has a meshblock id, which is the smaller block the ABS breaks addresses up into for SA1, SA2, SA3 and SA4's.\n",
    "\n",
    "This dataset is pretty useful for visualising the GNAF data for that reason.\n",
    "\n",
    "### Non Abs Main Structures \n",
    "\n",
    "We are mostly ingesting these to make it simpler to narrow data of interest. Typically if you're looking at this data, you're probably doing it some scope of relevance, such as a local government area, an electorate division, or whatever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aad15d-4c08-4bdd-adb3-0030e9d822a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.tasks.ingest_abs import ingest\n",
    "\n",
    "await ingest(gnaf_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98810d8-9626-4ed5-bd95-1ebf2fc45455",
   "metadata": {},
   "source": [
    "## Ingesting NSW Land Values\n",
    "\n",
    "First lets just get the CSV's into the database, then we'll break it up into seperates tables, then we'll form links with the GNAF dataset.\n",
    "\n",
    "#### Documentation on this dataset\n",
    "\n",
    "The valuer general website has a link to documentation on interpretting that data on [this page](https://www.nsw.gov.au/housing-and-construction/land-values-nsw/resource-library/land-value-information-user-guide). I didn't link to the PDF directly as it occasionally updated and a direct link is at risk of going stale.\n",
    "\n",
    "It's useful getting the meaning behind the codes and terms used in the bulk data.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. **Build the `nsw_valuer_general.raw_entries_lv` table**: Here we are\n",
    "   just loading the each file from the latest land value publication\n",
    "   with minimal changes, and a bit of sanitisizing.\n",
    "2. **Break CSV data into sepreate relations**, Just to break up the data\n",
    "   into more efficent representations of the data, and data that will be\n",
    "   easier to query, we're going to perform a series of queries against\n",
    "   the GNAF data before using it populate the tables we care about.\n",
    "3. **Parse contents of the property description**, The `property_description`\n",
    "   from the original valuer general data constains alot of information. The\n",
    "   most important of which is the land parcel or `lot/plan` information.\n",
    "   There is other information in there as well.\n",
    "4. If specified drop the raw entries consumed in step (tho the default is\n",
    "   to do exactly that. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a7cd26-b31a-43da-af78-83284cf39c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.tasks.ingest_nswvg_land_values import ingest_land_values, NswVgLandValueIngestionConfig\n",
    "\n",
    "await ingest_land_values(\n",
    "    NswVgLandValueIngestionConfig(\n",
    "        keep_raw=GLOBAL_FLAGS['drop_raw_nsw_valuer_general_entries'],\n",
    "    ),\n",
    "    gnaf_db,\n",
    "    environment.land_value.latest,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051a81ac-214d-44d5-9580-4be1a6260b33",
   "metadata": {},
   "source": [
    "### Break CSV data into sepreate relations\n",
    "\n",
    "Just to break up the data into more efficent representations of the data, and data that will be easier to query, we're going to perform a series of queries against the GNAF data before using it populate the tables we care about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c200d9b0-708a-4df7-8374-fd3e8409b85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sqlalchemy import text\n",
    "from psycopg2.errors import StringDataRightTruncation\n",
    "\n",
    "with gnaf_db.connect() as conn:\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS nsw_valuer_general.source_file CASCADE\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS nsw_valuer_general.source CASCADE\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS nsw_valuer_general.district CASCADE\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS nsw_valuer_general.suburb CASCADE\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS nsw_valuer_general.street CASCADE\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS nsw_valuer_general.property CASCADE\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS nsw_valuer_general.property_description CASCADE\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS nsw_valuer_general.valuations CASCADE\")\n",
    "    \n",
    "    with open('sql/nsw_lv_schema_2_structure.sql', 'r') as f:\n",
    "        cursor.execute(f.read())\n",
    "        \n",
    "    with open('sql/nsw_lv_from_raw.sql', 'r') as f:\n",
    "        cursor.execute(f.read())\n",
    "        \n",
    "    cursor.close()\n",
    "    \n",
    "count('district')\n",
    "count('suburb')\n",
    "count('street')\n",
    "count('property')\n",
    "count('property_description')\n",
    "count('valuations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e04d007-9c6d-4dc8-883e-b8907fb2aa28",
   "metadata": {},
   "source": [
    "### Parse contents of the property description\n",
    "\n",
    "The `property_description` from the original valuer general data constains alot of information. The most important of which is the land parcel or `lot/plan` information. There is other information in there as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aece36db-4b4b-4c5b-9ba9-b597d67abb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lib.nsw_vg.property_description import parse_land_parcel_ids\n",
    "\n",
    "engine = gnaf_db.engine()\n",
    "\n",
    "with gnaf_db.connect() as conn:\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS nsw_valuer_general.land_parcel_link\")\n",
    "    with open('sql/nsw_lv_schema_3_property_description_meta_data.sql', 'r') as f:\n",
    "        cursor.execute(f.read())\n",
    "    cursor.close()\n",
    "\n",
    "def land_parcels(desc):\n",
    "    desc, parcels = parse_land_parcel_ids(desc)\n",
    "    return parcels \n",
    "\n",
    "query = \"SELECT * FROM nsw_valuer_general.property_description\"\n",
    "for df_chunk in pd.read_sql(query, engine, chunksize=10000):\n",
    "    df_chunk = df_chunk.dropna(subset=['property_description'])\n",
    "    df_chunk['parcels'] = df_chunk['property_description'].apply(land_parcels)\n",
    "    df_chunk_ex = df_chunk.explode('parcels')\n",
    "    df_chunk_ex = df_chunk_ex.dropna(subset=['parcels'])\n",
    "    df_chunk_ex['land_parcel_id'] = df_chunk_ex['parcels'].apply(lambda p: p.id)\n",
    "    df_chunk_ex['part'] = df_chunk_ex['parcels'].apply(lambda p: p.part)\n",
    "    df_chunk_ex = df_chunk_ex.drop(columns=['property_description', 'parcels'])\n",
    "    df_chunk_ex.to_sql(\n",
    "        'land_parcel_link',\n",
    "        con=engine,\n",
    "        schema='nsw_valuer_general',\n",
    "        if_exists='append',\n",
    "        index=False,\n",
    "    )\n",
    "\n",
    "with gnaf_db.connect() as conn:\n",
    "    cursor = conn.cursor()\n",
    "    for t in ['property', 'land_parcel_link']:\n",
    "        cursor.execute(f'SELECT COUNT(*) FROM nsw_valuer_general.{t}')\n",
    "        count = cursor.fetchone()[0]\n",
    "        print(f\"Table nsw_valuer_general.{t} has {count} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f156c2-79d7-41a2-b98f-be2c6e2ef7d3",
   "metadata": {},
   "source": [
    "### Get rid of `raw_entries` table\n",
    "\n",
    "We no longer need the raw entries table, deleting it should make the database a bit efficent in terms of storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9adef8-bd79-4253-a71f-2edc9d1ddc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gnaf_db.connect() as conn:\n",
    "    cursor = conn.cursor()\n",
    "    if GLOBAL_FLAGS['drop_raw_nsw_valuer_general_entries']:\n",
    "        cursor.execute(\"DROP TABLE IF EXISTS nsw_valuer_general.raw_entries_lv\")\n",
    "        print(\"Dropping raw entries table\")\n",
    "    else:\n",
    "        print(\"Keeping raw entries table\")\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7205dcf6-90ba-4ddd-9a64-dd04b7d14cf9",
   "metadata": {},
   "source": [
    "## Gnaf Ingestion\n",
    "\n",
    "Here we ingest the GNAF dataset, this will take awhile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cc6440-562d-4468-b5d9-d3d53c925ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.gnaf.ingestion import ingest\n",
    "ingest(gnaf_dis.publication, gnaf_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2ce4e7-4133-492f-beb7-5920ed34835f",
   "metadata": {},
   "source": [
    "## Done\n",
    "\n",
    "We've now built up the dataset, lets analysis what we got and show the contents of the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e887af-b30e-40da-ae8b-ae0c6d779816",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gnaf_db.connect() as conn:\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    for schema in [\n",
    "        'nsw_valuer_general',\n",
    "        'gnaf',\n",
    "        'abs_main_structures',\n",
    "        'non_abs_main_structures',\n",
    "    ]:\n",
    "        # Get the list of all tables\n",
    "        cursor.execute(f\"\"\"\n",
    "            SELECT table_name\n",
    "            FROM information_schema.tables\n",
    "            WHERE table_schema = '{schema}'\n",
    "        \"\"\")\n",
    "        tables = cursor.fetchall()\n",
    "    \n",
    "        # Get row count for each table\n",
    "        for table in tables:\n",
    "            cursor.execute(f'SELECT COUNT(*) FROM {schema}.{table[0]}')\n",
    "            count = cursor.fetchone()[0]\n",
    "            print(f\"Table {schema}.{table[0]} has {count} rows\")\n",
    "    \n",
    "    cursor.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06f48d8-550c-4021-8a26-0def2c3e67af",
   "metadata": {},
   "source": [
    "## Todo\n",
    "\n",
    "Include NSW Planning data ([source 1][nswps1]) ([source 2][nswps2]) ([source 3][nswps3], examples [here][dom-example] discussion [here][planning-discussion])\n",
    "\n",
    "[nswps1]: https://portal.spatial.nsw.gov.au/server/rest/services/NSW_Land_Parcel_Property_Theme/FeatureServer\n",
    "[nswps2]: https://mapprod3.environment.nsw.gov.au/arcgis/rest/services/Planning\n",
    "[nswps3]: https://www.planningportal.nsw.gov.au/opendata/dataset/online-da-data-api\n",
    "[dom-example]: https://github.com/Dominic-Behrens/nsw_da_api\n",
    "[planning-discussion]: https://discord.com/channels/1099200773772034066/1099200773772034070/1270743511255220366"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dfc130-2baf-44b3-803a-8c37cf2ec288",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
