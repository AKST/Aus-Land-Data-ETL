{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e64a6769-685f-4029-8daa-4780d096ef81",
   "metadata": {},
   "source": [
    "# Dataset ingestion\n",
    "\n",
    "This jupyter noteebook ingests the [Geocoded National Address File][gnaf] ([GNAF][gnaf]) from [data.gov.au](data.gov.au). It also downloads the [land values for NSW][nswlv], and ABS shapefiles \n",
    "\n",
    "It loads it all this data into a PostgreSQL database in a docker container, treating it like a disposable sqlite data store. It also downloads the ABS shape files as well as the \n",
    "\n",
    "Here we are going to ingest all the data necessary in order to assess land by land values, and filter them by address information. \n",
    "\n",
    "### The Steps\n",
    "\n",
    "1. Download static assets and datasets\n",
    "2. Setup a docker container with postgresql with GIS capabilities.\n",
    "3. Ingest the [ABS shape files][abssf]\n",
    "4. Ingest the latest [NSW valuer general land values][nswlv].\n",
    "5. Ingest the [Geocoded National Address File][gnaf] ([GNAF][gnaf]) dataset\n",
    "6. Link NSW Valuer General data with GNAF dataset\n",
    "\n",
    "[gnaf]: https://data.gov.au/data/dataset/geocoded-national-address-file-g-naf\n",
    "[nswlv]: https://www.valuergeneral.nsw.gov.au/land_value_summaries/lv.php\n",
    "[abssf]: https://www.abs.gov.au/statistics/standards/australian-statistical-geography-standard-asgs-edition-3/jul2021-jun2026/access-and-downloads/digital-boundary-files\n",
    "\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "Here they are as a `requirements.txt`\n",
    "\n",
    "```\n",
    "aiopg==1.4.0\n",
    "asyncpg==0.29.0\n",
    "dask==2024.7.0\n",
    "dask-expr==1.1.7\n",
    "dask-geopandas==0.4.1\n",
    "docker==7.1.0\n",
    "fiona==1.9.6\n",
    "fuzzywuzzy==0.18.0\n",
    "GeoAlchemy2==0.15.2\n",
    "geopandas==1.0.1\n",
    "Levenshtein==0.25.1\n",
    "matplotlib==3.9.1\n",
    "numpy==2.0.0\n",
    "openpyxl==3.1.5\n",
    "OWSLib==0.31.0\n",
    "pandas==2.2.2\n",
    "psycopg2==2.9.9\n",
    "pyarrow==16.1.0\n",
    "pyperclip==1.9.0\n",
    "pyproj==3.6.1\n",
    "Rtree==1.3.0\n",
    "scikit-learn==1.5.1\n",
    "scipy==1.14.0\n",
    "shapely==2.0.4\n",
    "SQLAlchemy==2.0.31\n",
    "XlsxWriter==3.2.0\n",
    "```\n",
    "\n",
    "### Note\n",
    "\n",
    "- Make sure docker is running first.\n",
    "\n",
    "### Warning\n",
    "\n",
    "Do not connect this to another database unless you've taken the time to update this, as it'll drop the existing database. I suggest instead take what you need from this script and disregard the rest. DO NOT USE DATABASE CREDENTIALS HERE FOR ANY OTHER STORE (especailly anything with drop permissions).\n",
    "\n",
    "It also executes sql from a zip file downloaded from an external source.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a77f1d8-22ff-4aa2-8740-58dd6b05c461",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "These are some fields to configure if you wish to configure how the data is injected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab987146-dde7-4ddc-bd9c-916596d4badb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import notebook_constants as nc\n",
    "\n",
    "GLOBAL_FLAGS = {\n",
    "    # If you mark this as true, the table `nsw_valuer_general.raw_entries`\n",
    "    # will be dropped. If you have space limitations and no desire to debug\n",
    "    # the data than dropping this makes sense. If you wish to debug some values\n",
    "    # then keeping this around may make some sense.\n",
    "    'drop_raw_nsw_valuer_general_entries': True,\n",
    "}\n",
    "\n",
    "db_conf = nc.gnaf_dbconf\n",
    "db_name = nc.gnaf_dbname\n",
    "\n",
    "docker_container_name = 'gnaf_db_prod'\n",
    "docker_image_tag = \"20240821_18_52\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48db8607-3ea8-43c1-9b87-3af22555c2b4",
   "metadata": {},
   "source": [
    "## Download Static Files\n",
    "\n",
    "Here we are downloading static files, as well as fetching the most recently published land values from the valuer generals website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "320cd7fe-f03d-4230-9e0f-a2ef302a91b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gnaf-2020.zip\n",
      "Checking non_abs_shape.zip\n",
      "Checking cities.zip\n",
      "Checking nswvg_lv_01_Sep_2024.zip\n",
      "Checking nswvg_wps_01_Jan_2024.zip\n",
      "Checking nswvg_wps_08_Jan_2024.zip\n",
      "Checking nswvg_wps_15_Jan_2024.zip\n",
      "Checking nswvg_wps_22_Jan_2024.zip\n",
      "Checking nswvg_wps_29_Jan_2024.zip\n",
      "Checking nswvg_wps_05_Feb_2024.zip\n",
      "Checking nswvg_wps_12_Feb_2024.zip\n",
      "Checking nswvg_wps_19_Feb_2024.zip\n",
      "Checking nswvg_wps_26_Feb_2024.zip\n",
      "Checking nswvg_wps_04_Mar_2024.zip\n",
      "Checking nswvg_wps_11_Mar_2024.zip\n",
      "Checking nswvg_wps_18_Mar_2024.zip\n",
      "Checking nswvg_wps_25_Mar_2024.zip\n",
      "Checking nswvg_wps_01_Apr_2024.zip\n",
      "Checking nswvg_wps_08_Apr_2024.zip\n",
      "Checking nswvg_wps_15_Apr_2024.zip\n",
      "Checking nswvg_wps_22_Apr_2024.zip\n",
      "Checking nswvg_wps_29_Apr_2024.zip\n",
      "Checking nswvg_wps_06_May_2024.zip\n",
      "Checking nswvg_wps_13_May_2024.zip\n",
      "Checking nswvg_wps_20_May_2024.zip\n",
      "Checking nswvg_wps_27_May_2024.zip\n",
      "Checking nswvg_wps_03_Jun_2024.zip\n",
      "Checking nswvg_wps_10_Jun_2024.zip\n",
      "Checking nswvg_wps_17_Jun_2024.zip\n",
      "Checking nswvg_wps_24_Jun_2024.zip\n",
      "Checking nswvg_wps_01_Jul_2024.zip\n",
      "Checking nswvg_wps_08_Jul_2024.zip\n",
      "Checking nswvg_wps_15_Jul_2024.zip\n",
      "Checking nswvg_wps_22_Jul_2024.zip\n",
      "Checking nswvg_wps_29_Jul_2024.zip\n",
      "Checking nswvg_wps_05_Aug_2024.zip\n",
      "Checking nswvg_wps_12_Aug_2024.zip\n",
      "Checking nswvg_wps_19_Aug_2024.zip\n",
      "Checking nswvg_wps_26_Aug_2024.zip\n",
      "Checking nswvg_wps_02_Sep_2024.zip\n",
      "Checking nswvg_aps_1990.zip\n",
      "Checking nswvg_aps_1991.zip\n",
      "Checking nswvg_aps_1992.zip\n",
      "Checking nswvg_aps_1993.zip\n",
      "Checking nswvg_aps_1994.zip\n",
      "Checking nswvg_aps_1995.zip\n",
      "Checking nswvg_aps_1996.zip\n",
      "Checking nswvg_aps_1997.zip\n",
      "Checking nswvg_aps_1998.zip\n",
      "Checking nswvg_aps_1999.zip\n",
      "Checking nswvg_aps_2000.zip\n",
      "Checking nswvg_aps_2001.zip\n",
      "Checking nswvg_aps_2002.zip\n",
      "Checking nswvg_aps_2003.zip\n",
      "Checking nswvg_aps_2004.zip\n",
      "Checking nswvg_aps_2005.zip\n",
      "Checking nswvg_aps_2006.zip\n",
      "Checking nswvg_aps_2007.zip\n",
      "Checking nswvg_aps_2008.zip\n",
      "Checking nswvg_aps_2009.zip\n",
      "Checking nswvg_aps_2010.zip\n",
      "Checking nswvg_aps_2011.zip\n",
      "Checking nswvg_aps_2012.zip\n",
      "Checking nswvg_aps_2013.zip\n",
      "Checking nswvg_aps_2014.zip\n",
      "Checking nswvg_aps_2015.zip\n",
      "Checking nswvg_aps_2016.zip\n",
      "Checking nswvg_aps_2017.zip\n",
      "Checking nswvg_aps_2018.zip\n",
      "Checking nswvg_aps_2019.zip\n",
      "Checking nswvg_aps_2020.zip\n",
      "Checking nswvg_aps_2021.zip\n",
      "Checking nswvg_aps_2022.zip\n",
      "Checking nswvg_aps_2023.zip\n"
     ]
    }
   ],
   "source": [
    "from lib.nsw_vg import data_discovery as dd\n",
    "from lib.remote_resources import StaticFileInitialiser\n",
    "\n",
    "initialiser = StaticFileInitialiser.create()\n",
    "land_value = dd.LandValueDiscovery()\n",
    "w_sale_price = dd.WeeklySalePriceDiscovery()\n",
    "a_sale_price = dd.AnnualSalePriceDiscovery()\n",
    "\n",
    "lv_target = land_value.get_latest()\n",
    "if lv_target:\n",
    "    initialiser.add_target(lv_target)\n",
    "\n",
    "for sale_price_target in w_sale_price.get_links():\n",
    "    initialiser.add_target(sale_price_target)\n",
    "\n",
    "for sale_price_target in a_sale_price.get_links():\n",
    "    initialiser.add_target(sale_price_target)\n",
    "\n",
    "initialiser.setup_dirs()\n",
    "initialiser.fetch_remote_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bec1a4-8640-40ca-8c07-aafd1dd3a8e8",
   "metadata": {},
   "source": [
    "## Create Container with Database\n",
    "\n",
    "Here we are creating a container in docker from an image that uses the postgres image, which also installs a few extensions.\n",
    "\n",
    "### Note\n",
    "\n",
    "This notebook this is designed to be run more than once, so it'll throw away any existing container and database before creating a new one. After getting rid of any container using the same identifer, it'll create a new one and pull the relevant image if it's not already installed. It'll wait till the postgres instance is live then create the database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f709233-e243-468b-971e-df23bfcc0113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running zip-out/gnaf-2020/G-NAF/Extras/GNAF_TableCreation_Scripts/create_tables_ansi.sql\n",
      "running zip-out/gnaf-2020/G-NAF/Extras/GNAF_TableCreation_Scripts/add_fk_constraints.sql\n",
      "running sql/move_gnaf_to_schema.sql\n"
     ]
    }
   ],
   "source": [
    "from lib.gnaf_db import GnafDb, GnafContainer, GnafImage\n",
    "from lib import notebook_constants as nc\n",
    "\n",
    "image = GnafImage.create(tag=docker_image_tag)\n",
    "image.prepare()\n",
    "\n",
    "container = GnafContainer.create(container_name=docker_container_name, image=image)\n",
    "container.clean()\n",
    "container.prepare(db_conf, db_name)\n",
    "container.start()\n",
    "\n",
    "gnaf_db = GnafDb.create(db_conf, db_name)\n",
    "gnaf_db.wait_till_running()\n",
    "gnaf_db.init_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdd42a1-6d1e-4fe2-a3d3-20e7526dc021",
   "metadata": {},
   "source": [
    "## Consume the ABS Shapefiles\n",
    "\n",
    "The [ABS provides a number of shape files][all abs shape files], we're going focus on 2 main sets of shapes. The **ABS Main Structures** which is stuff like SA1, 2, 3 & 4 along with greater cities, meshblocks, and states. As well as **Non ABS Main Structures** which is stuff like electoral divisions, suburbs post codes etc.\n",
    "\n",
    "[all abs shape files]: https://www.abs.gov.au/statistics/standards/australian-statistical-geography-standard-asgs-edition-3/jul2021-jun2026/access-and-downloads/digital-boundary-files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edc026f-47c3-41d1-a390-a0e4a5a8ed5e",
   "metadata": {},
   "source": [
    "### ABS Main Structures \n",
    "\n",
    "Any address or region we look up in the GNAF dataset, we want to visualise. The ABS has a few different geographic groups which we can visualise the data against, but each address in the GNAF dataset has a meshblock id, which is the smaller block the ABS breaks addresses up into for SA1, SA2, SA3 and SA4's.\n",
    "\n",
    "This dataset is pretty useful for visualising the GNAF data for that reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32aad15d-4c08-4bdd-adb3-0030e9d822a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populated abs_main_structures.state with 10/10 rows.\n",
      "Populated abs_main_structures.gccsa with 35/35 rows.\n",
      "Populated abs_main_structures.sa4 with 108/108 rows.\n",
      "Populated abs_main_structures.sa3 with 359/359 rows.\n",
      "Populated abs_main_structures.sa2 with 2473/2473 rows.\n",
      "Populated abs_main_structures.sa1 with 61845/61845 rows.\n",
      "Populated abs_main_structures.meshblock with 368286/368286 rows.\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "    \n",
    "engine = gnaf_db.engine()\n",
    "\n",
    "schema = 'abs_main_structures'\n",
    "        \n",
    "column_renames_for_table = {\n",
    "    'SA1_2021_AUST_GDA2020': {\n",
    "        'SA1_CODE_2021': 'sa1_code', 'SA2_CODE_2021': 'sa2_code', 'SA3_CODE_2021': 'sa3_code',\n",
    "        'SA4_CODE_2021': 'sa4_code', 'GCCSA_CODE_2021': 'gcc_code', 'STATE_CODE_2021': 'state_code',\n",
    "        'AREA_ALBERS_SQKM': 'area_sqkm', 'geometry': 'geometry'\n",
    "    },\n",
    "    'SA2_2021_AUST_GDA2020': {\n",
    "        'SA2_CODE_2021': 'sa2_code', 'SA2_NAME_2021': 'sa2_name', 'SA3_CODE_2021': 'sa3_code',\n",
    "        'SA4_CODE_2021': 'sa4_code', 'GCCSA_CODE_2021': 'gcc_code', 'STATE_CODE_2021': 'state_code',\n",
    "        'AREA_ALBERS_SQKM': 'area_sqkm', 'geometry': 'geometry'\n",
    "    },\n",
    "    'SA3_2021_AUST_GDA2020': {\n",
    "        'SA3_CODE_2021': 'sa3_code', 'SA3_NAME_2021': 'sa3_name', 'SA4_CODE_2021': 'sa4_code',\n",
    "        'GCCSA_CODE_2021': 'gcc_code', 'STATE_CODE_2021': 'state_code', 'AREA_ALBERS_SQKM': 'area_sqkm',\n",
    "        'geometry': 'geometry'\n",
    "    },\n",
    "    'SA4_2021_AUST_GDA2020': {\n",
    "        'SA4_CODE_2021': 'sa4_code', 'SA4_NAME_2021': 'sa4_name', 'GCCSA_CODE_2021': 'gcc_code',\n",
    "        'STATE_CODE_2021': 'state_code', 'AREA_ALBERS_SQKM': 'area_sqkm', 'geometry': 'geometry'\n",
    "    },\n",
    "    'GCCSA_2021_AUST_GDA2020': {\n",
    "        'GCCSA_CODE_2021': 'gcc_code', 'GCCSA_NAME_2021': 'gcc_name', 'STATE_CODE_2021': 'state_code',\n",
    "        'geometry': 'geometry'\n",
    "    },\n",
    "    'STE_2021_AUST_GDA2020': {\n",
    "        'STATE_CODE_2021': 'state_code', 'STATE_NAME_2021': 'state_name', 'geometry': 'geometry'\n",
    "    },\n",
    "    'MB_2021_AUST_GDA2020': {\n",
    "        'MB_CODE_2021': 'mb_code', 'MB_CATEGORY_2021': 'mb_cat',\n",
    "        'SA1_CODE_2021': 'sa1_code', 'SA2_CODE_2021': 'sa2_code', 'SA3_CODE_2021': 'sa3_code',\n",
    "        'SA4_CODE_2021': 'sa4_code', 'GCCSA_CODE_2021': 'gcc_code', 'STATE_CODE_2021': 'state_code',\n",
    "        'AREA_ALBERS_SQKM': 'area_sqkm', 'geometry': 'geometry'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Column renames for each layer\n",
    "layers = {\n",
    "    'STE_2021_AUST_GDA2020': 'state',\n",
    "    'GCCSA_2021_AUST_GDA2020': 'gccsa',\n",
    "    'SA4_2021_AUST_GDA2020': 'sa4',\n",
    "    'SA3_2021_AUST_GDA2020': 'sa3',\n",
    "    'SA2_2021_AUST_GDA2020': 'sa2',\n",
    "    'SA1_2021_AUST_GDA2020': 'sa1',\n",
    "    'MB_2021_AUST_GDA2020': 'meshblock'\n",
    "}\n",
    "\n",
    "with gnaf_db.connect() as conn:\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # this really won't do anything unless you need to rerun this portion of the script\n",
    "    for _, table in layers.items():\n",
    "        cursor.execute(f\"\"\"\n",
    "        DO $$ BEGIN\n",
    "          IF EXISTS (\n",
    "            SELECT 1 FROM information_schema.tables \n",
    "             WHERE table_name = '{table}' AND table_schema = '{schema}'\n",
    "          ) THEN\n",
    "            TRUNCATE TABLE {schema}.{table} RESTART IDENTITY CASCADE;\n",
    "          END IF;\n",
    "        END $$;\n",
    "        \"\"\")\n",
    "    \n",
    "    with open('sql/abs_main_structures_create_tables.sql', 'r') as f:\n",
    "        cursor.execute(f.read())\n",
    "        \n",
    "    cursor.close()\n",
    "\n",
    "for layer_name, table_name in layers.items():\n",
    "    column_renames = column_renames_for_table[layer_name]\n",
    "    \n",
    "    # Load each layer into corresponding tables\n",
    "    df = gpd.read_file('zip-out/cities/ASGS_2021_MAIN_STRUCTURE_GDA2020.gpkg', layer=layer_name)\n",
    "    df = df.rename(columns=column_renames)\n",
    "    df = df[list(column_renames.values())]\n",
    "    df.to_postgis(table_name, engine, schema=schema, if_exists='append', index=False)\n",
    "\n",
    "    with engine.connect() as connection:\n",
    "        result = pd.read_sql(f\"SELECT COUNT(*) FROM {schema}.{table_name}\", connection)\n",
    "        print(f\"Populated {schema}.{table_name} with {result.iloc[0, 0]}/{len(df)} rows.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0312224f-3489-4501-840c-c9673269896a",
   "metadata": {},
   "source": [
    "### Non Abs Main Structures \n",
    "\n",
    "We are mostly ingesting these to make it simpler to narrow data of interest. Typically if you're looking at this data, you're probably doing it some scope of relevance, such as a local government area, an electorate division, or whatever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "540b58fa-73e9-4452-8e58-3521ab22de15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populated non_abs_main_structures.localities with 15353/15353 rows.\n",
      "Populated non_abs_main_structures.state_electoral_division_2021 with 452/452 rows.\n",
      "Populated non_abs_main_structures.state_electoral_division_2022 with 452/452 rows.\n",
      "Populated non_abs_main_structures.state_electoral_division_2024 with 452/452 rows.\n",
      "Populated non_abs_main_structures.federal_electoral_division_2021 with 170/170 rows.\n",
      "Populated non_abs_main_structures.lga_2021 with 566/566 rows.\n",
      "Populated non_abs_main_structures.lga_2022 with 566/566 rows.\n",
      "Populated non_abs_main_structures.lga_2023 with 566/566 rows.\n",
      "Populated non_abs_main_structures.lga_2024 with 566/566 rows.\n",
      "Populated non_abs_main_structures.post_code with 2644/2644 rows.\n",
      "Populated non_abs_main_structures.dzn with 9329/9329 rows.\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "schema = 'non_abs_main_structures'\n",
    "\n",
    "column_renames_for_table = {\n",
    "    'SAL_2021_AUST_GDA2020': {\n",
    "        \"SAL_CODE_2021\": \"locality_id\",\n",
    "        \"SAL_NAME_2021\": \"locality_name\",\n",
    "        \"STATE_CODE_2021\": \"state_code\",\n",
    "        \"AUS_CODE_2021\": \"in_australia\",\n",
    "        \"AREA_ALBERS_SQKM\": \"area_sqkm\",\n",
    "        \"geometry\": \"geometry\"\n",
    "    },\n",
    "    'SED_2021_AUST_GDA2020': {\n",
    "        \"SED_CODE_2021\": \"electorate_id\",\n",
    "        \"SED_NAME_2021\": \"electorate_name\",\n",
    "        \"STATE_CODE_2021\": \"state_code\",\n",
    "        \"AUS_CODE_2021\": \"in_australia\",\n",
    "        \"AREA_ALBERS_SQKM\": \"area_sqkm\",\n",
    "        \"geometry\": \"geometry\"\n",
    "    },\n",
    "    'SED_2022_AUST_GDA2020': {\n",
    "        \"SED_CODE_2022\": \"electorate_id\",\n",
    "        \"SED_NAME_2022\": \"electorate_name\",\n",
    "        \"STATE_CODE_2021\": \"state_code\",\n",
    "        \"AUS_CODE_2021\": \"in_australia\",\n",
    "        \"AREA_ALBERS_SQKM\": \"area_sqkm\",\n",
    "        \"geometry\": \"geometry\"\n",
    "    },\n",
    "    'SED_2024_AUST_GDA2020': {\n",
    "        \"SED_CODE_2024\": \"electorate_id\",\n",
    "        \"SED_NAME_2024\": \"electorate_name\",\n",
    "        \"STATE_CODE_2021\": \"state_code\",\n",
    "        \"AUS_CODE_2021\": \"in_australia\",\n",
    "        \"AREA_ALBERS_SQKM\": \"area_sqkm\",\n",
    "        \"geometry\": \"geometry\"\n",
    "    },\n",
    "    'CED_2021_AUST_GDA2020': {\n",
    "        \"CED_CODE_2021\": \"electorate_id\",\n",
    "        \"CED_NAME_2021\": \"electorate_name\",\n",
    "        \"STATE_CODE_2021\": \"state_code\",\n",
    "        \"AUS_CODE_2021\": \"in_australia\",\n",
    "        \"AREA_ALBERS_SQKM\": \"area_sqkm\",\n",
    "        \"geometry\": \"geometry\"\n",
    "    },\n",
    "    'LGA_2021_AUST_GDA2020': {\n",
    "        \"LGA_CODE_2021\": \"lga_id\",\n",
    "        \"LGA_NAME_2021\": \"lga_name\",\n",
    "        \"STATE_CODE_2021\": \"state_code\",\n",
    "        \"AUS_CODE_2021\": \"in_australia\",\n",
    "        \"AREA_ALBERS_SQKM\": \"area_sqkm\",\n",
    "        \"geometry\": \"geometry\"\n",
    "    },\n",
    "    'LGA_2022_AUST_GDA2020': {\n",
    "        \"LGA_CODE_2022\": \"lga_id\",\n",
    "        \"LGA_NAME_2022\": \"lga_name\",\n",
    "        \"STATE_CODE_2021\": \"state_code\",\n",
    "        \"AUS_CODE_2021\": \"in_australia\",\n",
    "        \"AREA_ALBERS_SQKM\": \"area_sqkm\",\n",
    "        \"geometry\": \"geometry\"\n",
    "    },\n",
    "    'LGA_2023_AUST_GDA2020': {\n",
    "        \"LGA_CODE_2023\": \"lga_id\",\n",
    "        \"LGA_NAME_2023\": \"lga_name\",\n",
    "        \"STATE_CODE_2021\": \"state_code\",\n",
    "        \"AUS_CODE_2021\": \"in_australia\",\n",
    "        \"AREA_ALBERS_SQKM\": \"area_sqkm\",\n",
    "        \"geometry\": \"geometry\"\n",
    "    },\n",
    "    'LGA_2024_AUST_GDA2020': {\n",
    "        \"LGA_CODE_2024\": \"lga_id\",\n",
    "        \"LGA_NAME_2024\": \"lga_name\",\n",
    "        \"STATE_CODE_2021\": \"state_code\",\n",
    "        \"AUS_CODE_2021\": \"in_australia\",\n",
    "        \"AREA_ALBERS_SQKM\": \"area_sqkm\",\n",
    "        \"geometry\": \"geometry\"\n",
    "    },\n",
    "    'POA_2021_AUST_GDA2020': {\n",
    "        \"POA_CODE_2021\": \"post_code\",\n",
    "        \"AUS_CODE_2021\": \"in_australia\",\n",
    "        \"AREA_ALBERS_SQKM\": \"area_sqkm\",\n",
    "        \"geometry\": \"geometry\"\n",
    "    },\n",
    "    # https://www.abs.gov.au/statistics/standards/australian-statistical-geography-standard-asgs-edition-3/jul2021-jun2026/non-abs-structures/destination-zones\n",
    "    'DZN_2021_AUST_GDA2020': {\n",
    "        'DZN_CODE_2021': 'dzn_code', \n",
    "        'SA2_CODE_2021': 'sa2_code',\n",
    "        'STATE_CODE_2021': 'state_code',\n",
    "        \"AUS_CODE_2021\": 'in_australia',\n",
    "        \"AREA_ALBERS_SQKM\": \"area_sqkm\",\n",
    "        \"geometry\": \"geometry\",\n",
    "    },\n",
    "}\n",
    "\n",
    "layers = {\n",
    "    'SAL_2021_AUST_GDA2020': 'localities',\n",
    "    'SED_2021_AUST_GDA2020': 'state_electoral_division_2021',\n",
    "    'SED_2022_AUST_GDA2020': 'state_electoral_division_2022',\n",
    "    'SED_2024_AUST_GDA2020': 'state_electoral_division_2024',\n",
    "    'CED_2021_AUST_GDA2020': 'federal_electoral_division_2021',\n",
    "    'LGA_2021_AUST_GDA2020': 'lga_2021',\n",
    "    'LGA_2022_AUST_GDA2020': 'lga_2022',\n",
    "    'LGA_2023_AUST_GDA2020': 'lga_2023',\n",
    "    'LGA_2024_AUST_GDA2020': 'lga_2024',\n",
    "    'POA_2021_AUST_GDA2020': 'post_code',\n",
    "    'DZN_2021_AUST_GDA2020': 'dzn',\n",
    "    # Unused\n",
    "    # - australian drainage divisions, 'ADD_2021_AUST_GDA2020'\n",
    "    # - tourism regions, 'TR_2021_AUST_GDA2020'\n",
    "}\n",
    "\n",
    "with gnaf_db.connect() as conn:\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # this really won't do anything unless you need to rerun this portion of the script\n",
    "    for _, table in layers.items():\n",
    "        cursor.execute(f\"\"\"\n",
    "        DO $$ BEGIN\n",
    "          IF EXISTS (\n",
    "            SELECT 1 FROM information_schema.tables \n",
    "             WHERE table_name = '{table}' AND table_schema = '{schema}'\n",
    "          ) THEN\n",
    "            TRUNCATE TABLE {schema}.{table} RESTART IDENTITY CASCADE;\n",
    "          END IF;\n",
    "        END $$;\n",
    "        \"\"\")\n",
    "    \n",
    "    with open('sql/non_abs_main_structures_create_tables.sql', 'r') as f:\n",
    "        cursor.execute(f.read())\n",
    "        \n",
    "    cursor.close()\n",
    "    \n",
    "for layer_name, table_name in layers.items():\n",
    "    column_renames = column_renames_for_table[layer_name]\n",
    "    \n",
    "    df = gpd.read_file('zip-out/non_abs_structures_shapefiles/ASGS_Ed3_Non_ABS_Structures_GDA2020_updated_2024.gpkg', layer=layer_name)\n",
    "    df = df.rename(columns=column_renames)\n",
    "    df = df[list(column_renames.values())]\n",
    "\n",
    "    if 'in_australia' in df:\n",
    "        df['in_australia'] = df['in_australia'] == 'AUS'\n",
    "    \n",
    "    df.to_postgis(table_name, engine, schema=schema, if_exists='append', index=False)\n",
    "\n",
    "    with engine.connect() as connection:\n",
    "        result = pd.read_sql(f\"SELECT COUNT(*) FROM {schema}.{table_name}\", connection)\n",
    "        print(f\"Populated {schema}.{table_name} with {result.iloc[0, 0]}/{len(df)} rows.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98810d8-9626-4ed5-bd95-1ebf2fc45455",
   "metadata": {},
   "source": [
    "## Ingesting Land Values\n",
    "\n",
    "First lets just get the CSV's into the database, then we'll break it up into seperates tables, then we'll form links with the GNAF dataset.\n",
    "\n",
    "### Documentation on this dataset\n",
    "\n",
    "The valuer general website has a link to documentation on interpretting that data on [this page](https://www.nsw.gov.au/housing-and-construction/land-values-nsw/resource-library/land-value-information-user-guide). I didn't link to the PDF directly as it occasionally updated and a direct link is at risk of going stale.\n",
    "\n",
    "It's useful getting the meaning behind the codes and terms used in the bulk data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002e5c53-ca27-4841-a386-4436e668b43e",
   "metadata": {},
   "source": [
    "### Build the `nsw_valuer_general.raw_entries` table\n",
    "\n",
    "Here we are just loading the each file from the latest land value publication with minimal changes, and a bit of sanitisizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34a7cd26-b31a-43da-af78-83284cf39c81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-01 18:06:55 Consumed zip-out/lv_01_Sep_2024/054_LAND_VALUE_DATA_20240901.csv, raw_entries 31492024-09-01 18:06:55 Consumed zip-out/lv_01_Sep_2024/052_LAND_VALUE_DATA_20240901.csv, raw_entries 3149\n",
      "\n",
      "2024-09-01 18:06:55 Consumed zip-out/lv_01_Sep_2024/043_LAND_VALUE_DATA_20240901.csv, raw_entries 3149\n",
      "2024-09-01 18:06:55 Consumed zip-out/lv_01_Sep_2024/051_LAND_VALUE_DATA_20240901.csv, raw_entries 3149\n",
      "2024-09-01 18:06:55 Consumed zip-out/lv_01_Sep_2024/061_LAND_VALUE_DATA_20240901.csv, raw_entries 3149\n",
      "2024-09-01 18:06:56 Consumed zip-out/lv_01_Sep_2024/066_LAND_VALUE_DATA_20240901.csv, raw_entries 5834\n",
      "2024-09-01 18:06:56 Consumed zip-out/lv_01_Sep_2024/065_LAND_VALUE_DATA_20240901.csv, raw_entries 5834\n",
      "2024-09-01 18:06:56 Consumed zip-out/lv_01_Sep_2024/070_LAND_VALUE_DATA_20240901.csv, raw_entries 9660\n",
      "2024-09-01 18:06:56 Consumed zip-out/lv_01_Sep_2024/083_LAND_VALUE_DATA_20240901.csv, raw_entries 13380\n",
      "2024-09-01 18:06:59 Consumed zip-out/lv_01_Sep_2024/002_LAND_VALUE_DATA_20240901.csv, raw_entries 18812\n",
      "2024-09-01 18:07:00 Consumed zip-out/lv_01_Sep_2024/084_LAND_VALUE_DATA_20240901.csv, raw_entries 52417\n",
      "2024-09-01 18:07:06 Consumed zip-out/lv_01_Sep_2024/042_LAND_VALUE_DATA_20240901.csv, raw_entries 59962\n",
      "2024-09-01 18:07:06 Consumed zip-out/lv_01_Sep_2024/007_LAND_VALUE_DATA_20240901.csv, raw_entries 67840\n",
      "2024-09-01 18:07:10 Consumed zip-out/lv_01_Sep_2024/012_LAND_VALUE_DATA_20240901.csv, raw_entries 110611\n",
      "2024-09-01 18:07:10 Consumed zip-out/lv_01_Sep_2024/010_LAND_VALUE_DATA_20240901.csv, raw_entries 110611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l1/1wl5vmds75qfv945_bs13x6w0000gn/T/ipykernel_31340/3306132734.py:31: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(full_file_path, encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-01 18:07:11 Consumed zip-out/lv_01_Sep_2024/085_LAND_VALUE_DATA_20240901.csv, raw_entries 118400\n",
      "2024-09-01 18:07:13 Consumed zip-out/lv_01_Sep_2024/005_LAND_VALUE_DATA_20240901.csv, raw_entries 118400\n",
      "2024-09-01 18:07:18 Consumed zip-out/lv_01_Sep_2024/074_LAND_VALUE_DATA_20240901.csv, raw_entries 192728\n",
      "2024-09-01 18:07:18 Consumed zip-out/lv_01_Sep_2024/081_LAND_VALUE_DATA_20240901.csv, raw_entries 192728\n",
      "2024-09-01 18:07:18 Consumed zip-out/lv_01_Sep_2024/001_LAND_VALUE_DATA_20240901.csv, raw_entries 222237\n",
      "2024-09-01 18:07:18 Consumed zip-out/lv_01_Sep_2024/088_LAND_VALUE_DATA_20240901.csv, raw_entries 222237\n",
      "2024-09-01 18:07:20 Consumed zip-out/lv_01_Sep_2024/087_LAND_VALUE_DATA_20240901.csv, raw_entries 222237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l1/1wl5vmds75qfv945_bs13x6w0000gn/T/ipykernel_31340/3306132734.py:31: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(full_file_path, encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-01 18:07:30 Consumed zip-out/lv_01_Sep_2024/082_LAND_VALUE_DATA_20240901.csv, raw_entries 263721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l1/1wl5vmds75qfv945_bs13x6w0000gn/T/ipykernel_31340/3306132734.py:31: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(full_file_path, encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-01 18:07:31 Consumed zip-out/lv_01_Sep_2024/117_LAND_VALUE_DATA_20240901.csv, raw_entries 386924\n",
      "2024-09-01 18:07:31 Consumed zip-out/lv_01_Sep_2024/004_LAND_VALUE_DATA_20240901.csv, raw_entries 386924\n",
      "2024-09-01 18:07:31 Consumed zip-out/lv_01_Sep_2024/109_LAND_VALUE_DATA_20240901.csv, raw_entries 386924\n",
      "2024-09-01 18:07:31 Consumed zip-out/lv_01_Sep_2024/050_LAND_VALUE_DATA_20240901.csv, raw_entries 386924\n",
      "2024-09-01 18:07:32 Consumed zip-out/lv_01_Sep_2024/090_LAND_VALUE_DATA_20240901.csv, raw_entries 386924\n",
      "2024-09-01 18:07:33 Consumed zip-out/lv_01_Sep_2024/098_LAND_VALUE_DATA_20240901.csv, raw_entries 405084\n",
      "2024-09-01 18:07:34 Consumed zip-out/lv_01_Sep_2024/116_LAND_VALUE_DATA_20240901.csv, raw_entries 405084\n",
      "2024-09-01 18:07:36 Consumed zip-out/lv_01_Sep_2024/123_LAND_VALUE_DATA_20240901.csv, raw_entries 409018\n",
      "2024-09-01 18:07:36 Consumed zip-out/lv_01_Sep_2024/100_LAND_VALUE_DATA_20240901.csv, raw_entries 441987\n",
      "2024-09-01 18:07:37 Consumed zip-out/lv_01_Sep_2024/118_LAND_VALUE_DATA_20240901.csv, raw_entries 441987\n",
      "2024-09-01 18:07:38 Consumed zip-out/lv_01_Sep_2024/092_LAND_VALUE_DATA_20240901.csv, raw_entries 441987\n",
      "2024-09-01 18:07:39 Consumed zip-out/lv_01_Sep_2024/102_LAND_VALUE_DATA_20240901.csv, raw_entries 466814\n",
      "2024-09-01 18:07:41 Consumed zip-out/lv_01_Sep_2024/137_LAND_VALUE_DATA_20240901.csv, raw_entries 481926\n",
      "2024-09-01 18:07:41 Consumed zip-out/lv_01_Sep_2024/143_LAND_VALUE_DATA_20240901.csv, raw_entries 481926\n",
      "2024-09-01 18:07:41 Consumed zip-out/lv_01_Sep_2024/018_LAND_VALUE_DATA_20240901.csv, raw_entries 501409\n",
      "2024-09-01 18:07:42 Consumed zip-out/lv_01_Sep_2024/008_LAND_VALUE_DATA_20240901.csv, raw_entries 501409\n",
      "2024-09-01 18:07:44 Consumed zip-out/lv_01_Sep_2024/149_LAND_VALUE_DATA_20240901.csv, raw_entries 507783\n",
      "2024-09-01 18:07:46 Consumed zip-out/lv_01_Sep_2024/150_LAND_VALUE_DATA_20240901.csv, raw_entries 532898\n",
      "2024-09-01 18:07:46 Consumed zip-out/lv_01_Sep_2024/097_LAND_VALUE_DATA_20240901.csv, raw_entries 543469\n",
      "2024-09-01 18:07:47 Consumed zip-out/lv_01_Sep_2024/151_LAND_VALUE_DATA_20240901.csv, raw_entries 543469\n",
      "2024-09-01 18:07:47 Consumed zip-out/lv_01_Sep_2024/139_LAND_VALUE_DATA_20240901.csv, raw_entries 543469\n",
      "2024-09-01 18:07:50 Consumed zip-out/lv_01_Sep_2024/124_LAND_VALUE_DATA_20240901.csv, raw_entries 629197\n",
      "2024-09-01 18:07:50 Consumed zip-out/lv_01_Sep_2024/158_LAND_VALUE_DATA_20240901.csv, raw_entries 568310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l1/1wl5vmds75qfv945_bs13x6w0000gn/T/ipykernel_31340/3306132734.py:31: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(full_file_path, encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-01 18:07:51 Consumed zip-out/lv_01_Sep_2024/101_LAND_VALUE_DATA_20240901.csv, raw_entries 646110\n",
      "2024-09-01 18:07:51 Consumed zip-out/lv_01_Sep_2024/148_LAND_VALUE_DATA_20240901.csv, raw_entries 646110\n",
      "2024-09-01 18:07:52 Consumed zip-out/lv_01_Sep_2024/157_LAND_VALUE_DATA_20240901.csv, raw_entries 671186\n",
      "2024-09-01 18:07:52 Consumed zip-out/lv_01_Sep_2024/164_LAND_VALUE_DATA_20240901.csv, raw_entries 671186\n",
      "2024-09-01 18:07:52 Consumed zip-out/lv_01_Sep_2024/187_LAND_VALUE_DATA_20240901.csv, raw_entries 6809162024-09-01 18:07:52 Consumed zip-out/lv_01_Sep_2024/199_LAND_VALUE_DATA_20240901.csv, raw_entries 680916\n",
      "\n",
      "2024-09-01 18:07:53 Consumed zip-out/lv_01_Sep_2024/192_LAND_VALUE_DATA_20240901.csv, raw_entries 687721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l1/1wl5vmds75qfv945_bs13x6w0000gn/T/ipykernel_31340/3306132734.py:31: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(full_file_path, encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-01 18:07:53 Consumed zip-out/lv_01_Sep_2024/188_LAND_VALUE_DATA_20240901.csv, raw_entries 696653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l1/1wl5vmds75qfv945_bs13x6w0000gn/T/ipykernel_31340/3306132734.py:31: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(full_file_path, encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-01 18:07:56 Consumed zip-out/lv_01_Sep_2024/152_LAND_VALUE_DATA_20240901.csv, raw_entries 7155652024-09-01 18:07:56 Consumed zip-out/lv_01_Sep_2024/159_LAND_VALUE_DATA_20240901.csv, raw_entries 715565\n",
      "\n",
      "2024-09-01 18:07:58 Consumed zip-out/lv_01_Sep_2024/210_LAND_VALUE_DATA_20240901.csv, raw_entries 728713\n",
      "2024-09-01 18:07:59 Consumed zip-out/lv_01_Sep_2024/209_LAND_VALUE_DATA_20240901.csv, raw_entries 728713\n",
      "2024-09-01 18:08:02 Consumed zip-out/lv_01_Sep_2024/222_LAND_VALUE_DATA_20240901.csv, raw_entries 740848\n",
      "2024-09-01 18:08:04 Consumed zip-out/lv_01_Sep_2024/171_LAND_VALUE_DATA_20240901.csv, raw_entries 772756\n",
      "2024-09-01 18:08:05 Consumed zip-out/lv_01_Sep_2024/144_LAND_VALUE_DATA_20240901.csv, raw_entries 774401\n",
      "2024-09-01 18:08:05 Consumed zip-out/lv_01_Sep_2024/230_LAND_VALUE_DATA_20240901.csv, raw_entries 774401\n",
      "2024-09-01 18:08:05 Consumed zip-out/lv_01_Sep_2024/207_LAND_VALUE_DATA_20240901.csv, raw_entries 774401\n",
      "2024-09-01 18:08:05 Consumed zip-out/lv_01_Sep_2024/231_LAND_VALUE_DATA_20240901.csv, raw_entries 778827\n",
      "2024-09-01 18:08:06 Consumed zip-out/lv_01_Sep_2024/232_LAND_VALUE_DATA_20240901.csv, raw_entries 780983\n",
      "2024-09-01 18:08:08 Consumed zip-out/lv_01_Sep_2024/233_LAND_VALUE_DATA_20240901.csv, raw_entries 782142\n",
      "2024-09-01 18:08:10 Consumed zip-out/lv_01_Sep_2024/235_LAND_VALUE_DATA_20240901.csv, raw_entries 787264\n",
      "2024-09-01 18:08:10 Consumed zip-out/lv_01_Sep_2024/236_LAND_VALUE_DATA_20240901.csv, raw_entries 787264\n",
      "2024-09-01 18:08:12 Consumed zip-out/lv_01_Sep_2024/219_LAND_VALUE_DATA_20240901.csv, raw_entries 816230\n",
      "2024-09-01 18:08:13 Consumed zip-out/lv_01_Sep_2024/238_LAND_VALUE_DATA_20240901.csv, raw_entries 889348\n",
      "2024-09-01 18:08:13 Consumed zip-out/lv_01_Sep_2024/103_LAND_VALUE_DATA_20240901.csv, raw_entries 889348\n",
      "2024-09-01 18:08:13 Consumed zip-out/lv_01_Sep_2024/239_LAND_VALUE_DATA_20240901.csv, raw_entries 889348\n",
      "2024-09-01 18:08:15 Consumed zip-out/lv_01_Sep_2024/234_LAND_VALUE_DATA_20240901.csv, raw_entries 9017902024-09-01 18:08:15 Consumed zip-out/lv_01_Sep_2024/243_LAND_VALUE_DATA_20240901.csv, raw_entries 901790\n",
      "\n",
      "2024-09-01 18:08:16 Consumed zip-out/lv_01_Sep_2024/226_LAND_VALUE_DATA_20240901.csv, raw_entries 9266872024-09-01 18:08:16 Consumed zip-out/lv_01_Sep_2024/240_LAND_VALUE_DATA_20240901.csv, raw_entries 926687\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l1/1wl5vmds75qfv945_bs13x6w0000gn/T/ipykernel_31340/3306132734.py:31: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(full_file_path, encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-01 18:08:19 Consumed zip-out/lv_01_Sep_2024/216_LAND_VALUE_DATA_20240901.csv, raw_entries 963936\n",
      "2024-09-01 18:08:19 Consumed zip-out/lv_01_Sep_2024/252_LAND_VALUE_DATA_20240901.csv, raw_entries 970343\n",
      "2024-09-01 18:08:19 Consumed zip-out/lv_01_Sep_2024/244_LAND_VALUE_DATA_20240901.csv, raw_entries 970343\n",
      "2024-09-01 18:08:21 Consumed zip-out/lv_01_Sep_2024/251_LAND_VALUE_DATA_20240901.csv, raw_entries 981113\n",
      "2024-09-01 18:08:21 Consumed zip-out/lv_01_Sep_2024/247_LAND_VALUE_DATA_20240901.csv, raw_entries 981113\n",
      "2024-09-01 18:08:22 Consumed zip-out/lv_01_Sep_2024/253_LAND_VALUE_DATA_20240901.csv, raw_entries 10400402024-09-01 18:08:22 Consumed zip-out/lv_01_Sep_2024/250_LAND_VALUE_DATA_20240901.csv, raw_entries 1040040\n",
      "\n",
      "2024-09-01 18:08:22 Consumed zip-out/lv_01_Sep_2024/217_LAND_VALUE_DATA_20240901.csv, raw_entries 1040040\n",
      "2024-09-01 18:08:22 Consumed zip-out/lv_01_Sep_2024/254_LAND_VALUE_DATA_20240901.csv, raw_entries 1040040\n",
      "2024-09-01 18:08:24 Consumed zip-out/lv_01_Sep_2024/255_LAND_VALUE_DATA_20240901.csv, raw_entries 1044628\n",
      "2024-09-01 18:08:28 Consumed zip-out/lv_01_Sep_2024/257_LAND_VALUE_DATA_20240901.csv, raw_entries 1057817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l1/1wl5vmds75qfv945_bs13x6w0000gn/T/ipykernel_31340/3306132734.py:31: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(full_file_path, encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-01 18:08:28 Consumed zip-out/lv_01_Sep_2024/262_LAND_VALUE_DATA_20240901.csv, raw_entries 1063047\n",
      "2024-09-01 18:08:30 Consumed zip-out/lv_01_Sep_2024/265_LAND_VALUE_DATA_20240901.csv, raw_entries 1069790\n",
      "2024-09-01 18:08:31 Consumed zip-out/lv_01_Sep_2024/218_LAND_VALUE_DATA_20240901.csv, raw_entries 1181772\n",
      "2024-09-01 18:08:31 Consumed zip-out/lv_01_Sep_2024/220_LAND_VALUE_DATA_20240901.csv, raw_entries 1189683\n",
      "2024-09-01 18:08:32 Consumed zip-out/lv_01_Sep_2024/263_LAND_VALUE_DATA_20240901.csv, raw_entries 1189683\n",
      "2024-09-01 18:08:33 Consumed zip-out/lv_01_Sep_2024/266_LAND_VALUE_DATA_20240901.csv, raw_entries 1201460\n",
      "2024-09-01 18:08:35 Consumed zip-out/lv_01_Sep_2024/270_LAND_VALUE_DATA_20240901.csv, raw_entries 1204056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l1/1wl5vmds75qfv945_bs13x6w0000gn/T/ipykernel_31340/3306132734.py:31: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(full_file_path, encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-01 18:08:37 Consumed zip-out/lv_01_Sep_2024/269_LAND_VALUE_DATA_20240901.csv, raw_entries 1212967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l1/1wl5vmds75qfv945_bs13x6w0000gn/T/ipykernel_31340/3306132734.py:31: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(full_file_path, encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-01 18:08:41 Consumed zip-out/lv_01_Sep_2024/274_LAND_VALUE_DATA_20240901.csv, raw_entries 1222328\n",
      "2024-09-01 18:08:42 Consumed zip-out/lv_01_Sep_2024/273_LAND_VALUE_DATA_20240901.csv, raw_entries 12367322024-09-01 18:08:42 Consumed zip-out/lv_01_Sep_2024/223_LAND_VALUE_DATA_20240901.csv, raw_entries 1236732\n",
      "\n",
      "2024-09-01 18:08:44 Consumed zip-out/lv_01_Sep_2024/224_LAND_VALUE_DATA_20240901.csv, raw_entries 1304081\n",
      "2024-09-01 18:08:47 Consumed zip-out/lv_01_Sep_2024/300_LAND_VALUE_DATA_20240901.csv, raw_entries 1307411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l1/1wl5vmds75qfv945_bs13x6w0000gn/T/ipykernel_31340/3306132734.py:31: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(full_file_path, encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-01 18:08:50 Consumed zip-out/lv_01_Sep_2024/214_LAND_VALUE_DATA_20240901.csv, raw_entries 1307411\n",
      "2024-09-01 18:08:52 Consumed zip-out/lv_01_Sep_2024/301_LAND_VALUE_DATA_20240901.csv, raw_entries 1311974\n",
      "2024-09-01 18:08:53 Consumed zip-out/lv_01_Sep_2024/264_LAND_VALUE_DATA_20240901.csv, raw_entries 1373499\n",
      "2024-09-01 18:08:54 Consumed zip-out/lv_01_Sep_2024/302_LAND_VALUE_DATA_20240901.csv, raw_entries 1378965\n",
      "2024-09-01 18:08:54 Consumed zip-out/lv_01_Sep_2024/272_LAND_VALUE_DATA_20240901.csv, raw_entries 1378965\n",
      "2024-09-01 18:08:56 Consumed zip-out/lv_01_Sep_2024/260_LAND_VALUE_DATA_20240901.csv, raw_entries 1378965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l1/1wl5vmds75qfv945_bs13x6w0000gn/T/ipykernel_31340/3306132734.py:31: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(full_file_path, encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-01 18:08:57 Consumed zip-out/lv_01_Sep_2024/261_LAND_VALUE_DATA_20240901.csv, raw_entries 1378965\n",
      "2024-09-01 18:08:59 Consumed zip-out/lv_01_Sep_2024/511_LAND_VALUE_DATA_20240901.csv, raw_entries 1411897\n",
      "2024-09-01 18:08:59 Consumed zip-out/lv_01_Sep_2024/275_LAND_VALUE_DATA_20240901.csv, raw_entries 1411897\n",
      "2024-09-01 18:09:00 Consumed zip-out/lv_01_Sep_2024/526_LAND_VALUE_DATA_20240901.csv, raw_entries 1418893\n",
      "2024-09-01 18:09:00 Consumed zip-out/lv_01_Sep_2024/276_LAND_VALUE_DATA_20240901.csv, raw_entries 1418893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l1/1wl5vmds75qfv945_bs13x6w0000gn/T/ipykernel_31340/3306132734.py:31: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(full_file_path, encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-01 18:09:03 Consumed zip-out/lv_01_Sep_2024/538_LAND_VALUE_DATA_20240901.csv, raw_entries 1422802\n",
      "2024-09-01 18:09:04 Consumed zip-out/lv_01_Sep_2024/528_LAND_VALUE_DATA_20240901.csv, raw_entries 1429186\n",
      "2024-09-01 18:09:04 Consumed zip-out/lv_01_Sep_2024/537_LAND_VALUE_DATA_20240901.csv, raw_entries 1436359\n",
      "2024-09-01 18:09:04 Consumed zip-out/lv_01_Sep_2024/560_LAND_VALUE_DATA_20240901.csv, raw_entries 1436359\n",
      "2024-09-01 18:09:04 Consumed zip-out/lv_01_Sep_2024/268_LAND_VALUE_DATA_20240901.csv, raw_entries 1436359\n",
      "2024-09-01 18:09:06 Consumed zip-out/lv_01_Sep_2024/267_LAND_VALUE_DATA_20240901.csv, raw_entries 1487878\n",
      "2024-09-01 18:09:08 Consumed zip-out/lv_01_Sep_2024/529_LAND_VALUE_DATA_20240901.csv, raw_entries 1487878\n",
      "2024-09-01 18:09:10 Consumed zip-out/lv_01_Sep_2024/620_LAND_VALUE_DATA_20240901.csv, raw_entries 1487878\n",
      "2024-09-01 18:09:11 Consumed zip-out/lv_01_Sep_2024/303_LAND_VALUE_DATA_20240901.csv, raw_entries 1514927\n",
      "2024-09-01 18:09:12 Consumed zip-out/lv_01_Sep_2024/271_LAND_VALUE_DATA_20240901.csv, raw_entries 1514927\n",
      "2024-09-01 18:09:17 Consumed zip-out/lv_01_Sep_2024/608_LAND_VALUE_DATA_20240901.csv, raw_entries 1534990\n",
      "2024-09-01 18:09:19 Consumed zip-out/lv_01_Sep_2024/708_LAND_VALUE_DATA_20240901.csv, raw_entries 1564230\n",
      "2024-09-01 18:09:21 Consumed zip-out/lv_01_Sep_2024/575_LAND_VALUE_DATA_20240901.csv, raw_entries 1564230\n",
      "2024-09-01 18:09:22 Consumed zip-out/lv_01_Sep_2024/258_LAND_VALUE_DATA_20240901.csv, raw_entries 1653751\n",
      "2024-09-01 18:09:23 Consumed zip-out/lv_01_Sep_2024/666_LAND_VALUE_DATA_20240901.csv, raw_entries 1683069\n",
      "2024-09-01 18:09:24 Consumed zip-out/lv_01_Sep_2024/656_LAND_VALUE_DATA_20240901.csv, raw_entries 1717878\n",
      "2024-09-01 18:09:31 Consumed zip-out/lv_01_Sep_2024/259_LAND_VALUE_DATA_20240901.csv, raw_entries 1846599\n"
     ]
    },
    {
     "ename": "IntegrityError",
     "evalue": "(psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint \"pg_type_typname_nsp_index\"\nDETAIL:  Key (typname, typnamespace)=(raw_entries, 40843) already exists.\n\n[SQL: \nCREATE TABLE nsw_valuer_general.raw_entries (\n\tsource_file_position BIGINT, \n\tdistrict_code BIGINT, \n\tdistrict_name TEXT, \n\tproperty_id BIGINT, \n\tproperty_type TEXT, \n\tproperty_name TEXT, \n\tunit_number FLOAT(53), \n\thouse_number TEXT, \n\tstreet_name TEXT, \n\tsuburb_name TEXT, \n\tpostcode TEXT, \n\tproperty_description TEXT, \n\tzone_code TEXT, \n\tarea FLOAT(53), \n\tarea_type TEXT, \n\tbase_date_1 TEXT, \n\tland_value_1 BIGINT, \n\tauthority_1 TEXT, \n\tbasis_1 TEXT, \n\tbase_date_2 TEXT, \n\tland_value_2 BIGINT, \n\tauthority_2 TEXT, \n\tbasis_2 TEXT, \n\tbase_date_3 TEXT, \n\tland_value_3 FLOAT(53), \n\tauthority_3 TEXT, \n\tbasis_3 TEXT, \n\tbase_date_4 TEXT, \n\tland_value_4 FLOAT(53), \n\tauthority_4 TEXT, \n\tbasis_4 TEXT, \n\tbase_date_5 TEXT, \n\tland_value_5 FLOAT(53), \n\tauthority_5 TEXT, \n\tbasis_5 TEXT, \n\tsource_file_name TEXT, \n\tsource_date TIMESTAMP WITHOUT TIME ZONE\n)\n\n]\n(Background on this error at: https://sqlalche.me/e/20/gkpj)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUniqueViolation\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/code/jupyter/env/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1967\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[0;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[1;32m   1966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[0;32m-> 1967\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1968\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m   1969\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n",
      "File \u001b[0;32m~/code/jupyter/env/lib/python3.11/site-packages/sqlalchemy/engine/default.py:924\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 924\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mUniqueViolation\u001b[0m: duplicate key value violates unique constraint \"pg_type_typname_nsp_index\"\nDETAIL:  Key (typname, typnamespace)=(raw_entries, 40843) already exists.\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mIntegrityError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m futures \u001b[38;5;241m=\u001b[39m [executor\u001b[38;5;241m.\u001b[39msubmit(process_file, file) \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files]\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m as_completed(futures):\n\u001b[0;32m---> 55\u001b[0m     \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/lib/python3.11/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "Cell \u001b[0;32mIn[6], line 46\u001b[0m, in \u001b[0;36mprocess_file\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m     43\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpostcode\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [(n \u001b[38;5;28;01mif\u001b[39;00m math\u001b[38;5;241m.\u001b[39misnan(n) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mint\u001b[39m(n))) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpostcode\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 46\u001b[0m     \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mraw_entries\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgnaf_db\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnsw_valuer_general\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m     count(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_entries\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConsumed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/code/jupyter/env/lib/python3.11/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/jupyter/env/lib/python3.11/site-packages/pandas/core/generic.py:3087\u001b[0m, in \u001b[0;36mNDFrame.to_sql\u001b[0;34m(self, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\u001b[0m\n\u001b[1;32m   2889\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2890\u001b[0m \u001b[38;5;124;03mWrite records stored in a DataFrame to a SQL database.\u001b[39;00m\n\u001b[1;32m   2891\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3083\u001b[0m \u001b[38;5;124;03m[(1,), (None,), (2,)]\u001b[39;00m\n\u001b[1;32m   3084\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m   3085\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sql\n\u001b[0;32m-> 3087\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_sql\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3088\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3092\u001b[0m \u001b[43m    \u001b[49m\u001b[43mif_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_exists\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3098\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/jupyter/env/lib/python3.11/site-packages/pandas/io/sql.py:842\u001b[0m, in \u001b[0;36mto_sql\u001b[0;34m(frame, name, con, schema, if_exists, index, index_label, chunksize, dtype, method, engine, **engine_kwargs)\u001b[0m\n\u001b[1;32m    837\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    838\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m argument should be either a Series or a DataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    839\u001b[0m     )\n\u001b[1;32m    841\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con, schema\u001b[38;5;241m=\u001b[39mschema, need_transaction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[0;32m--> 842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_sql\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_exists\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/jupyter/env/lib/python3.11/site-packages/pandas/io/sql.py:2008\u001b[0m, in \u001b[0;36mSQLDatabase.to_sql\u001b[0;34m(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype, method, engine, **engine_kwargs)\u001b[0m\n\u001b[1;32m   1958\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1959\u001b[0m \u001b[38;5;124;03mWrite records stored in a DataFrame to a SQL database.\u001b[39;00m\n\u001b[1;32m   1960\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;124;03m    Any additional kwargs are passed to the engine.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2006\u001b[0m sql_engine \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[0;32m-> 2008\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprep_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2010\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mif_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_exists\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2014\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2016\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2018\u001b[0m total_inserted \u001b[38;5;241m=\u001b[39m sql_engine\u001b[38;5;241m.\u001b[39minsert_records(\n\u001b[1;32m   2019\u001b[0m     table\u001b[38;5;241m=\u001b[39mtable,\n\u001b[1;32m   2020\u001b[0m     con\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcon,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2027\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mengine_kwargs,\n\u001b[1;32m   2028\u001b[0m )\n\u001b[1;32m   2030\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_case_sensitive(name\u001b[38;5;241m=\u001b[39mname, schema\u001b[38;5;241m=\u001b[39mschema)\n",
      "File \u001b[0;32m~/code/jupyter/env/lib/python3.11/site-packages/pandas/io/sql.py:1912\u001b[0m, in \u001b[0;36mSQLDatabase.prep_table\u001b[0;34m(self, frame, name, if_exists, index, index_label, schema, dtype)\u001b[0m\n\u001b[1;32m   1900\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe type of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a SQLAlchemy type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1902\u001b[0m table \u001b[38;5;241m=\u001b[39m SQLTable(\n\u001b[1;32m   1903\u001b[0m     name,\n\u001b[1;32m   1904\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1910\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1911\u001b[0m )\n\u001b[0;32m-> 1912\u001b[0m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m table\n",
      "File \u001b[0;32m~/code/jupyter/env/lib/python3.11/site-packages/pandas/io/sql.py:995\u001b[0m, in \u001b[0;36mSQLTable.create\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    993\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mif_exists\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not valid for if_exists\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 995\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/jupyter/env/lib/python3.11/site-packages/pandas/io/sql.py:981\u001b[0m, in \u001b[0;36mSQLTable._execute_create\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtable\u001b[38;5;241m.\u001b[39mto_metadata(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpd_sql\u001b[38;5;241m.\u001b[39mmeta)\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpd_sql\u001b[38;5;241m.\u001b[39mrun_transaction():\n\u001b[0;32m--> 981\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpd_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcon\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/jupyter/env/lib/python3.11/site-packages/sqlalchemy/sql/schema.py:1289\u001b[0m, in \u001b[0;36mTable.create\u001b[0;34m(self, bind, checkfirst)\u001b[0m\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\u001b[38;5;28mself\u001b[39m, bind: _CreateDropBind, checkfirst: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Issue a ``CREATE`` statement for this\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;124;03m    :class:`_schema.Table`, using the given\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;124;03m    :class:`.Connection` or :class:`.Engine`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1286\u001b[0m \n\u001b[1;32m   1287\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1289\u001b[0m     \u001b[43mbind\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_ddl_visitor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mddl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSchemaGenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckfirst\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/jupyter/env/lib/python3.11/site-packages/sqlalchemy/engine/base.py:2457\u001b[0m, in \u001b[0;36mConnection._run_ddl_visitor\u001b[0;34m(self, visitorcallable, element, **kwargs)\u001b[0m\n\u001b[1;32m   2445\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_ddl_visitor\u001b[39m(\n\u001b[1;32m   2446\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2447\u001b[0m     visitorcallable: Type[Union[SchemaGenerator, SchemaDropper]],\n\u001b[1;32m   2448\u001b[0m     element: SchemaItem,\n\u001b[1;32m   2449\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   2450\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2451\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"run a DDL visitor.\u001b[39;00m\n\u001b[1;32m   2452\u001b[0m \n\u001b[1;32m   2453\u001b[0m \u001b[38;5;124;03m    This method is only here so that the MockConnection can change the\u001b[39;00m\n\u001b[1;32m   2454\u001b[0m \u001b[38;5;124;03m    options given to the visitor so that \"checkfirst\" is skipped.\u001b[39;00m\n\u001b[1;32m   2455\u001b[0m \n\u001b[1;32m   2456\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2457\u001b[0m     \u001b[43mvisitorcallable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraverse_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/jupyter/env/lib/python3.11/site-packages/sqlalchemy/sql/visitors.py:664\u001b[0m, in \u001b[0;36mExternalTraversal.traverse_single\u001b[0;34m(self, obj, **kw)\u001b[0m\n\u001b[1;32m    662\u001b[0m meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(v, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvisit_\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m obj\u001b[38;5;241m.\u001b[39m__visit_name__, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m meth:\n\u001b[0;32m--> 664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/jupyter/env/lib/python3.11/site-packages/sqlalchemy/sql/ddl.py:956\u001b[0m, in \u001b[0;36mSchemaGenerator.visit_table\u001b[0;34m(self, table, create_ok, include_foreign_key_constraints, _is_metadata_operation)\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdialect\u001b[38;5;241m.\u001b[39msupports_alter:\n\u001b[1;32m    948\u001b[0m     \u001b[38;5;66;03m# e.g., don't omit any foreign key constraints\u001b[39;00m\n\u001b[1;32m    949\u001b[0m     include_foreign_key_constraints \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    951\u001b[0m \u001b[43mCreateTable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_foreign_key_constraints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_foreign_key_constraints\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m--> 956\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke_with\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(table, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindexes\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m table\u001b[38;5;241m.\u001b[39mindexes:\n",
      "File \u001b[0;32m~/code/jupyter/env/lib/python3.11/site-packages/sqlalchemy/sql/ddl.py:314\u001b[0m, in \u001b[0;36mExecutableDDLElement._invoke_with\u001b[0;34m(self, bind)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_invoke_with\u001b[39m(\u001b[38;5;28mself\u001b[39m, bind):\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_execute(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget, bind):\n\u001b[0;32m--> 314\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbind\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/jupyter/env/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1418\u001b[0m, in \u001b[0;36mConnection.execute\u001b[0;34m(self, statement, parameters, execution_options)\u001b[0m\n\u001b[1;32m   1416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObjectNotExecutableError(statement) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1419\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mNO_OPTIONS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1422\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/jupyter/env/lib/python3.11/site-packages/sqlalchemy/sql/ddl.py:180\u001b[0m, in \u001b[0;36mExecutableDDLElement._execute_on_connection\u001b[0;34m(self, connection, distilled_params, execution_options)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_execute_on_connection\u001b[39m(\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28mself\u001b[39m, connection, distilled_params, execution_options\n\u001b[1;32m    179\u001b[0m ):\n\u001b[0;32m--> 180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_ddl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistilled_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_options\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/jupyter/env/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1529\u001b[0m, in \u001b[0;36mConnection._execute_ddl\u001b[0;34m(self, ddl, distilled_parameters, execution_options)\u001b[0m\n\u001b[1;32m   1524\u001b[0m dialect \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdialect\n\u001b[1;32m   1526\u001b[0m compiled \u001b[38;5;241m=\u001b[39m ddl\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m   1527\u001b[0m     dialect\u001b[38;5;241m=\u001b[39mdialect, schema_translate_map\u001b[38;5;241m=\u001b[39mschema_translate_map\n\u001b[1;32m   1528\u001b[0m )\n\u001b[0;32m-> 1529\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_ctx_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_ddl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexec_opts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n\u001b[1;32m   1538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mafter_execute(\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1540\u001b[0m         ddl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1544\u001b[0m         ret,\n\u001b[1;32m   1545\u001b[0m     )\n",
      "File \u001b[0;32m~/code/jupyter/env/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1846\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[1;32m   1844\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exec_insertmany_context(dialect, context)\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1846\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_exec_single_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/jupyter/env/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1986\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[0;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[1;32m   1983\u001b[0m     result \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39m_setup_result_proxy()\n\u001b[1;32m   1985\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1986\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_dbapi_exception\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1987\u001b[0m \u001b[43m        \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m   1988\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1990\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/code/jupyter/env/lib/python3.11/site-packages/sqlalchemy/engine/base.py:2353\u001b[0m, in \u001b[0;36mConnection._handle_dbapi_exception\u001b[0;34m(self, e, statement, parameters, cursor, context, is_sub_exec)\u001b[0m\n\u001b[1;32m   2351\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m should_wrap:\n\u001b[1;32m   2352\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m sqlalchemy_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2353\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m sqlalchemy_exception\u001b[38;5;241m.\u001b[39mwith_traceback(exc_info[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   2354\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2355\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m exc_info[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/code/jupyter/env/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1967\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[0;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[1;32m   1965\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[0;32m-> 1967\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1968\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m   1969\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n\u001b[1;32m   1972\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mafter_cursor_execute(\n\u001b[1;32m   1973\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1974\u001b[0m         cursor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1978\u001b[0m         context\u001b[38;5;241m.\u001b[39mexecutemany,\n\u001b[1;32m   1979\u001b[0m     )\n",
      "File \u001b[0;32m~/code/jupyter/env/lib/python3.11/site-packages/sqlalchemy/engine/default.py:924\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 924\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIntegrityError\u001b[0m: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint \"pg_type_typname_nsp_index\"\nDETAIL:  Key (typname, typnamespace)=(raw_entries, 40843) already exists.\n\n[SQL: \nCREATE TABLE nsw_valuer_general.raw_entries (\n\tsource_file_position BIGINT, \n\tdistrict_code BIGINT, \n\tdistrict_name TEXT, \n\tproperty_id BIGINT, \n\tproperty_type TEXT, \n\tproperty_name TEXT, \n\tunit_number FLOAT(53), \n\thouse_number TEXT, \n\tstreet_name TEXT, \n\tsuburb_name TEXT, \n\tpostcode TEXT, \n\tproperty_description TEXT, \n\tzone_code TEXT, \n\tarea FLOAT(53), \n\tarea_type TEXT, \n\tbase_date_1 TEXT, \n\tland_value_1 BIGINT, \n\tauthority_1 TEXT, \n\tbasis_1 TEXT, \n\tbase_date_2 TEXT, \n\tland_value_2 BIGINT, \n\tauthority_2 TEXT, \n\tbasis_2 TEXT, \n\tbase_date_3 TEXT, \n\tland_value_3 FLOAT(53), \n\tauthority_3 TEXT, \n\tbasis_3 TEXT, \n\tbase_date_4 TEXT, \n\tland_value_4 FLOAT(53), \n\tauthority_4 TEXT, \n\tbasis_4 TEXT, \n\tbase_date_5 TEXT, \n\tland_value_5 FLOAT(53), \n\tauthority_5 TEXT, \n\tbasis_5 TEXT, \n\tsource_file_name TEXT, \n\tsource_date TIMESTAMP WITHOUT TIME ZONE\n)\n\n]\n(Background on this error at: https://sqlalche.me/e/20/gkpj)"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sqlalchemy import text\n",
    "from psycopg2.errors import StringDataRightTruncation\n",
    "\n",
    "from lib import notebook_constants as nc\n",
    "\n",
    "with gnaf_db.connect() as conn:\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS nsw_valuer_general.raw_entries CASCADE\")\n",
    "    with open('sql/nsw_lv_schema_1_raw.sql', 'r') as f:\n",
    "        cursor.execute(f.read())\n",
    "    cursor.close()\n",
    "            \n",
    "column_mappings = { **nc.lv_long_column_mappings, **nc.lv_wide_columns_mappings }\n",
    "\n",
    "def count(table, source = None):\n",
    "    c = pd.read_sql(f'SELECT count(*) FROM nsw_valuer_general.{table}', gnaf_db.engine())\n",
    "    time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f'{time} {source and f\"{source}, \" or \"\"}{table} {c.iloc[0,0]}')\n",
    "\n",
    "def process_file(file):\n",
    "    if not file.endswith(\"csv\"):\n",
    "        return\n",
    "\n",
    "    full_file_path = f\"zip-out/{land_value_target.zip_dst}/{file}\"\n",
    "    try:\n",
    "        df = pd.read_csv(full_file_path, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        # Fallback to ISO-8859-1 encoding if utf-8 fails\n",
    "        df = pd.read_csv(full_file_path, encoding='ISO-8859-1')\n",
    "\n",
    "    date_str = file.split('_')[-1].replace('.csv', '')\n",
    "    \n",
    "    df.index.name = 'source_file_position'\n",
    "    df = df.drop(columns=['Unnamed: 34'])\n",
    "    df = df.rename(columns=column_mappings).reset_index()\n",
    "    df['source_file_name'] = file\n",
    "    df['source_date'] = datetime.strptime(date_str, \"%Y%m%d\")\n",
    "    df['postcode'] = [(n if math.isnan(n) else str(int(n))) for n in df['postcode']]\n",
    "    \n",
    "    try:\n",
    "        df.to_sql('raw_entries', gnaf_db.engine(), schema='nsw_valuer_general', if_exists='append', index=False)\n",
    "    finally:\n",
    "        count('raw_entries', f'Consumed {full_file_path}')\n",
    "\n",
    "files = sorted(os.listdir(f\"zip-out/{land_value_target.zip_dst}\"))\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "    futures = [executor.submit(process_file, file) for file in files]\n",
    "    for future in as_completed(futures):\n",
    "        future.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051a81ac-214d-44d5-9580-4be1a6260b33",
   "metadata": {},
   "source": [
    "### Break CSV data into sepreate relations\n",
    "\n",
    "Just to break up the data into more efficent representations of the data, and data that will be easier to query, we're going to perform a series of queries against the GNAF data before using it populate the tables we care about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c200d9b0-708a-4df7-8374-fd3e8409b85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sqlalchemy import text\n",
    "from psycopg2.errors import StringDataRightTruncation\n",
    "\n",
    "with gnaf_db.connect() as conn:\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS nsw_valuer_general.source_file CASCADE\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS nsw_valuer_general.source CASCADE\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS nsw_valuer_general.district CASCADE\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS nsw_valuer_general.suburb CASCADE\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS nsw_valuer_general.street CASCADE\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS nsw_valuer_general.property CASCADE\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS nsw_valuer_general.property_description CASCADE\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS nsw_valuer_general.valuations CASCADE\")\n",
    "    \n",
    "    with open('sql/nsw_lv_schema_2_structure.sql', 'r') as f:\n",
    "        cursor.execute(f.read())\n",
    "        \n",
    "    with open('sql/nsw_lv_from_raw.sql', 'r') as f:\n",
    "        cursor.execute(f.read())\n",
    "        \n",
    "    cursor.close()\n",
    "    \n",
    "count('district')\n",
    "count('suburb')\n",
    "count('street')\n",
    "count('property')\n",
    "count('property_description')\n",
    "count('valuations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e04d007-9c6d-4dc8-883e-b8907fb2aa28",
   "metadata": {},
   "source": [
    "### Parse contents of the property description\n",
    "\n",
    "The `property_description` from the original valuer general data constains alot of information. The most important of which is the land parcel or `lot/plan` information. There is other information in there as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aece36db-4b4b-4c5b-9ba9-b597d67abb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lib.nsw_vg.property_description import parse_land_parcel_ids\n",
    "\n",
    "engine = gnaf_db.engine()\n",
    "\n",
    "with gnaf_db.connect() as conn:\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS nsw_valuer_general.land_parcel_link\")\n",
    "    with open('sql/nsw_lv_schema_3_property_description_meta_data.sql', 'r') as f:\n",
    "        cursor.execute(f.read())\n",
    "    cursor.close()\n",
    "\n",
    "def land_parcels(desc):\n",
    "    desc, parcels = parse_land_parcel_ids(desc)\n",
    "    return parcels \n",
    "\n",
    "query = \"SELECT * FROM nsw_valuer_general.property_description\"\n",
    "for df_chunk in pd.read_sql(query, engine, chunksize=10000):\n",
    "    df_chunk = df_chunk.dropna(subset=['property_description'])\n",
    "    df_chunk['parcels'] = df_chunk['property_description'].apply(land_parcels)\n",
    "    df_chunk_ex = df_chunk.explode('parcels')\n",
    "    df_chunk_ex = df_chunk_ex.dropna(subset=['parcels'])\n",
    "    df_chunk_ex['land_parcel_id'] = df_chunk_ex['parcels'].apply(lambda p: p.id)\n",
    "    df_chunk_ex['part'] = df_chunk_ex['parcels'].apply(lambda p: p.part)\n",
    "    df_chunk_ex = df_chunk_ex.drop(columns=['property_description', 'parcels'])\n",
    "    df_chunk_ex.to_sql(\n",
    "        'land_parcel_link',\n",
    "        con=engine,\n",
    "        schema='nsw_valuer_general',\n",
    "        if_exists='append',\n",
    "        index=False,\n",
    "    )\n",
    "\n",
    "with gnaf_db.connect() as conn:\n",
    "    cursor = conn.cursor()\n",
    "    for t in ['property', 'land_parcel_link']:\n",
    "        cursor.execute(f'SELECT COUNT(*) FROM nsw_valuer_general.{t}')\n",
    "        count = cursor.fetchone()[0]\n",
    "        print(f\"Table nsw_valuer_general.{t} has {count} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f156c2-79d7-41a2-b98f-be2c6e2ef7d3",
   "metadata": {},
   "source": [
    "### Get rid of `raw_entries` table\n",
    "\n",
    "We no longer need the raw entries table, deleting it should make the database a bit efficent in terms of storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9adef8-bd79-4253-a71f-2edc9d1ddc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gnaf_db.connect() as conn:\n",
    "    cursor = conn.cursor()\n",
    "    if GLOBAL_FLAGS['drop_raw_nsw_valuer_general_entries']:\n",
    "        cursor.execute(\"DROP TABLE IF EXISTS nsw_valuer_general.raw_entries_lv\")\n",
    "        print(\"Dropping raw entries table\")\n",
    "    else:\n",
    "        print(\"Keeping raw entries table\")\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7205dcf6-90ba-4ddd-9a64-dd04b7d14cf9",
   "metadata": {},
   "source": [
    "## Gnaf Ingestion\n",
    "\n",
    "The main thing to consider with ingesting this data is the order in which it is ingested. Now you could actually add the foreign key constraints after populating the database, and go nuts (That might actually even be faster than what I got here). But after a day of different variants of this script while trying to juggle correctness of data ingested and speed, I'm settling for this.\n",
    "\n",
    "So 90% of the code here is just coordinating the dependencies and order in which everything is ingested, as well as doing as much in parallel as possible. My earlier approach of doing everything sequentially took 6 hours, is between 1-2 hours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cc6440-562d-4468-b5d9-d3d53c925ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "import psycopg2\n",
    "import concurrent.futures\n",
    "\n",
    "from collections import defaultdict, deque\n",
    "from datetime import datetime\n",
    "from lib import notebook_constants as nc\n",
    "from threading import Lock\n",
    "\n",
    "schema = 'gnaf'\n",
    "WORKER_COUNT = 32\n",
    "BATCH_SIZE = 2000\n",
    "\n",
    "def get_table_name(file):\n",
    "    file = os.path.splitext(os.path.basename(file))[0]\n",
    "    sidx = 15 if file.startswith('Authority_Code') else file.find('_')+1\n",
    "    return file[sidx:file.rfind('_')]\n",
    "\n",
    "def get_batches(batch_size, reader):\n",
    "    batch = []\n",
    "    for row in reader:\n",
    "        row = [(None if v == \"\" else v) for v in (v.strip() for v in row)]\n",
    "        batch.append(row)\n",
    "        \n",
    "        if len(batch) >= batch_size:\n",
    "            yield batch\n",
    "            batch = [] \n",
    "    if batch:\n",
    "        yield batch\n",
    "\n",
    "def populate_file(file):\n",
    "    table_name = get_table_name(file)\n",
    "    with gnaf_db.connect() as conn:\n",
    "        cursor = conn.cursor()\n",
    "        with open(file, 'r') as f:\n",
    "            time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            print(f\"{time} Populating from {os.path.basename(file)}\")\n",
    "            reader = csv.reader(f, delimiter='|')\n",
    "            headers = next(reader)\n",
    "            insert_query = f\"\"\"\n",
    "                INSERT INTO {schema}.{table_name} ({', '.join(headers)}) \n",
    "                VALUES ({', '.join(['%s'] * len(headers))})\n",
    "                ON CONFLICT DO NOTHING\n",
    "            \"\"\"\n",
    "            \n",
    "            for batch_index, batch in enumerate(get_batches(BATCH_SIZE, reader)):\n",
    "                try:\n",
    "                    cursor.executemany(insert_query, batch)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error inserting batch {batch_index + 1} into {table_name}: {e}\")\n",
    "                    raise e\n",
    "            conn.commit()\n",
    "\n",
    "def get_ordered_files(dependencies, all_files):\n",
    "    file_sizes = { file: os.path.getsize(file) for file in all_files }\n",
    "    total_blocked_sizes = defaultdict(int)\n",
    "    \n",
    "    def dfs(file, visited):\n",
    "        if file in visited:\n",
    "            return 0\n",
    "        visited.add(file)\n",
    "        total_size = file_sizes[file]\n",
    "        for dep in dependencies[file]:\n",
    "            total_size += dfs(dep, visited)\n",
    "        return total_size\n",
    "    \n",
    "    for file in dependencies:\n",
    "        visited = set()\n",
    "        total_blocked_sizes[file] = dfs(file, visited)\n",
    "        \n",
    "    # Sort files based on the total blocked sizes, with higher sizes first\n",
    "    return sorted(all_files, key=lambda f: total_blocked_sizes[f], reverse=True)\n",
    "\n",
    "def worker(file_queue, all_files, dependency_count, lock, dependency_completed):\n",
    "    while True:\n",
    "        with lock:\n",
    "            if not file_queue:\n",
    "                break\n",
    "            file = file_queue.popleft()\n",
    "        \n",
    "        populate_file(file)\n",
    "        \n",
    "        with lock:\n",
    "            dependency_completed.add(file)\n",
    "            \n",
    "            for d in dependency_count:\n",
    "                if file in dependency_count[d]:\n",
    "                    dependency_count[d].remove(file)\n",
    "                    \n",
    "            ready_files = [f for f in all_files if not dependency_count[f]]\n",
    "            \n",
    "            for ready_file in ready_files:\n",
    "                if ready_file not in file_queue:\n",
    "                    file_queue.append(ready_file)\n",
    "                    all_files.remove(ready_file)\n",
    "\n",
    "authority_files = glob.glob('zip-out/gnaf-2020/G-NAF/G-NAF MAY 2024/Authority Code/*.psv')\n",
    "\n",
    "standard_prefix = 'zip-out/gnaf-2020/G-NAF/G-NAF MAY 2024/Standard'\n",
    "standard_files = [\n",
    "    f'{standard_prefix}/{s}_{t}_psv.psv' \n",
    "    for t in [\n",
    "        'STATE', 'ADDRESS_SITE', 'MB_2016', 'MB_2021', 'LOCALITY',\n",
    "        'LOCALITY_ALIAS', 'LOCALITY_NEIGHBOUR', 'LOCALITY_POINT',\n",
    "        'STREET_LOCALITY', 'STREET_LOCALITY_ALIAS', 'STREET_LOCALITY_POINT',\n",
    "        'ADDRESS_DETAIL', 'ADDRESS_SITE_GEOCODE', 'ADDRESS_ALIAS', \n",
    "        'ADDRESS_DEFAULT_GEOCODE', 'ADDRESS_FEATURE', \n",
    "        'ADDRESS_MESH_BLOCK_2016', 'ADDRESS_MESH_BLOCK_2021',\n",
    "        'PRIMARY_SECONDARY',\n",
    "    ] \n",
    "    for s in ['NSW', 'VIC', 'QLD', 'WA', 'SA', 'TAS', 'NT', 'OT', 'ACT'] \n",
    "]\n",
    "\n",
    "standard_deps = { f: set() for f in authority_files } | { \n",
    "    f'{p}/{s}_{t}_psv.psv': { f'{p}/{s}_{d}_psv.psv' for d in ds } | set(authority_files)\n",
    "    \n",
    "    for t, ds in ({\n",
    "        'STATE': [],\n",
    "        'ADDRESS_SITE': ['STATE'],\n",
    "        'MB_2016': [],\n",
    "        'MB_2021': [],\n",
    "        'LOCALITY': ['STATE'],\n",
    "        'LOCALITY_ALIAS': ['LOCALITY'],\n",
    "        'LOCALITY_NEIGHBOUR': ['LOCALITY'],\n",
    "        'LOCALITY_POINT': ['LOCALITY'],\n",
    "        'STREET_LOCALITY': ['LOCALITY'],\n",
    "        'STREET_LOCALITY_ALIAS': ['STREET_LOCALITY'],\n",
    "        'STREET_LOCALITY_POINT': ['STREET_LOCALITY'],\n",
    "        'ADDRESS_DETAIL': ['ADDRESS_SITE', 'STATE', 'LOCALITY', 'STREET_LOCALITY'],\n",
    "        'ADDRESS_SITE_GEOCODE': ['ADDRESS_SITE'],\n",
    "        'ADDRESS_ALIAS': ['ADDRESS_DETAIL'],\n",
    "        'ADDRESS_DEFAULT_GEOCODE': ['ADDRESS_DETAIL'],\n",
    "        'ADDRESS_FEATURE': ['ADDRESS_DETAIL'],\n",
    "        'ADDRESS_MESH_BLOCK_2016': ['ADDRESS_DETAIL', 'MB_2016'],\n",
    "        'ADDRESS_MESH_BLOCK_2021': ['ADDRESS_DETAIL', 'MB_2021'],\n",
    "        'PRIMARY_SECONDARY': ['ADDRESS_DETAIL'],\n",
    "    }).items()\n",
    "    for s in ['NSW', 'VIC', 'QLD', 'WA', 'SA', 'TAS', 'NT', 'OT', 'ACT'] \n",
    "    for p in [standard_prefix]\n",
    "}\n",
    "\n",
    "lock = Lock()\n",
    "all_files = { *authority_files, *standard_files }\n",
    "dependency_count = {k: set(v) for k, v in standard_deps.items()}\n",
    "dependency_completed = set()\n",
    "file_queue = deque()\n",
    "file_queue.extend(f for f in get_ordered_files(dependency_count, all_files) if not dependency_count[f])\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=WORKER_COUNT) as executor:\n",
    "    futures = [executor.submit(worker, file_queue, all_files, dependency_count, lock, dependency_completed) for _ in range(WORKER_COUNT)]\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        future.result()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2ce4e7-4133-492f-beb7-5920ed34835f",
   "metadata": {},
   "source": [
    "## Done\n",
    "\n",
    "We've now built up the dataset, lets analysis what we got and show the contents of the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e887af-b30e-40da-ae8b-ae0c6d779816",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gnaf_db.connect() as conn:\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    for schema in [\n",
    "        'nsw_valuer_general',\n",
    "        'gnaf',\n",
    "        'abs_main_structures',\n",
    "        'non_abs_main_structures',\n",
    "    ]:\n",
    "        # Get the list of all tables\n",
    "        cursor.execute(f\"\"\"\n",
    "            SELECT table_name\n",
    "            FROM information_schema.tables\n",
    "            WHERE table_schema = '{schema}'\n",
    "        \"\"\")\n",
    "        tables = cursor.fetchall()\n",
    "    \n",
    "        # Get row count for each table\n",
    "        for table in tables:\n",
    "            cursor.execute(f'SELECT COUNT(*) FROM {schema}.{table[0]}')\n",
    "            count = cursor.fetchone()[0]\n",
    "            print(f\"Table {schema}.{table[0]} has {count} rows\")\n",
    "    \n",
    "    cursor.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06f48d8-550c-4021-8a26-0def2c3e67af",
   "metadata": {},
   "source": [
    "## Todo\n",
    "\n",
    "Include NSW Planning data ([source 1][nswps1]) ([source 2][nswps2]) ([source 3][nswps3], examples [here][dom-example] discussion [here][planning-discussion])\n",
    "\n",
    "[nswps1]: https://portal.spatial.nsw.gov.au/server/rest/services/NSW_Land_Parcel_Property_Theme/FeatureServer\n",
    "[nswps2]: https://mapprod3.environment.nsw.gov.au/arcgis/rest/services/Planning\n",
    "[nswps3]: https://www.planningportal.nsw.gov.au/opendata/dataset/online-da-data-api\n",
    "[dom-example]: https://github.com/Dominic-Behrens/nsw_da_api\n",
    "[planning-discussion]: https://discord.com/channels/1099200773772034066/1099200773772034070/1270743511255220366"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dfc130-2baf-44b3-803a-8c37cf2ec288",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
